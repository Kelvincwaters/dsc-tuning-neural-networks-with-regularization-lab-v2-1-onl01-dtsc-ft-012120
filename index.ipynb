{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll use a train-test partition as well as a validation set to get better insights about how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. From there, you'll define and compile the model like before. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Apply early stopping criteria with a neural network \n",
    "- Apply L1, L2, and dropout regularization on a neural network  \n",
    "- Examine the effects of training with more data on a neural network  \n",
    "\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "Run the following cell to import some of the libraries and classes you'll need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:56.204972Z",
     "start_time": "2020-05-12T14:45:45.550424Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in the file `'Bank_complaints.csv'`. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:56.900235Z",
     "start_time": "2020-05-12T14:45:56.206973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools such as regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* Train - test split\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels \n",
    "\n",
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training neural networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your model's performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "- Generate a random sample of 10,000 observations using seed 123 for consistency of results. \n",
    "- Split this sample into `X` and `y` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:56.910268Z",
     "start_time": "2020-05-12T14:45:56.902235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Downsample the data\n",
    "df_sample = df.sample(10000, random_state= 123)\n",
    "\n",
    "# Split the data into X and y\n",
    "y = df_sample.Product\n",
    "X = df_sample['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "- Split the data into training and test sets \n",
    "- Assign 1500 obervations to the test set and use 42 as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:56.918215Z",
     "start_time": "2020-05-12T14:45:56.912216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1500, random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set \n",
    "\n",
    "As mentioned in the previous lesson, it is good practice to set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test set can then be used to determine an unbiased perforance of the model. \n",
    "\n",
    "Run the cell below to further divide the training data into training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:56.926215Z",
     "start_time": "2020-05-12T14:45:56.920219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing before building a neural network model. \n",
    "\n",
    "- Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "- Transform the training, validate, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:59.889542Z",
     "start_time": "2020-05-12T14:45:56.927215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use one-hot encoding to reformat the complaints into a matrix of vectors \n",
    "# Only keep the 2000 most common words \n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_matrix(X_train_final, mode='binary')\n",
    "X_val_tokens = tokenizer.texts_to_matrix(X_val, mode='binary')\n",
    "X_test_tokens = tokenizer.texts_to_matrix(X_test, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero. \n",
    "\n",
    "Transform the training, validate, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:45:59.921158Z",
     "start_time": "2020-05-12T14:45:59.890541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the product labels to numerical values\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final)\n",
    "\n",
    "y_train_lb = to_categorical(lb.transform(y_train_final))[:, :, 1]\n",
    "y_val_lb = to_categorical(lb.transform(y_val))[:, :, 1]\n",
    "y_test_lb = to_categorical(lb.transform(y_test))[:, :, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Baseline Model \n",
    "\n",
    "Rebuild a fully connected (Dense) layer network:  \n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions (since you are dealing with a multiclass problem, classifying the complaints into 7 classes) \n",
    "- Use a `'softmax'` activation function for the output layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:00.006224Z",
     "start_time": "2020-05-12T14:45:59.923156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a baseline neural network model using Keras\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "baseline_model = models.Sequential()\n",
    "baseline_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "baseline_model.add(layers.Dense(25, activation='relu'))\n",
    "baseline_model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Compile this model with: \n",
    "\n",
    "- a stochastic gradient descent optimizer \n",
    "- `'categorical_crossentropy'` as the loss function \n",
    "- a focus on `'accuracy'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:00.053536Z",
     "start_time": "2020-05-12T14:46:00.008225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "baseline_model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "- Train the model for 150 epochs in mini-batches of 256 samples \n",
    "- Include the `validation_data` argument to ensure you keep track of the validation loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:29.128504Z",
     "start_time": "2020-05-12T14:46:00.054536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9721 - accuracy: 0.1261 - val_loss: 1.9469 - val_accuracy: 0.1660\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9474 - accuracy: 0.1513 - val_loss: 1.9321 - val_accuracy: 0.1900\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9332 - accuracy: 0.1679 - val_loss: 1.9215 - val_accuracy: 0.2060\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9208 - accuracy: 0.1852 - val_loss: 1.9112 - val_accuracy: 0.2160\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9081 - accuracy: 0.2064 - val_loss: 1.8997 - val_accuracy: 0.2330\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8939 - accuracy: 0.2287 - val_loss: 1.8863 - val_accuracy: 0.2570\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8771 - accuracy: 0.2572 - val_loss: 1.8692 - val_accuracy: 0.2790\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8564 - accuracy: 0.2871 - val_loss: 1.8475 - val_accuracy: 0.2950\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8313 - accuracy: 0.3188 - val_loss: 1.8209 - val_accuracy: 0.3200\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8014 - accuracy: 0.3452 - val_loss: 1.7901 - val_accuracy: 0.3670\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7671 - accuracy: 0.3831 - val_loss: 1.7560 - val_accuracy: 0.3880\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7281 - accuracy: 0.4113 - val_loss: 1.7161 - val_accuracy: 0.4180\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6844 - accuracy: 0.4409 - val_loss: 1.6753 - val_accuracy: 0.4290\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6374 - accuracy: 0.4701 - val_loss: 1.6274 - val_accuracy: 0.4630\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5868 - accuracy: 0.4984 - val_loss: 1.5778 - val_accuracy: 0.4930\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5340 - accuracy: 0.5273 - val_loss: 1.5265 - val_accuracy: 0.5030\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4805 - accuracy: 0.5463 - val_loss: 1.4755 - val_accuracy: 0.5240\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4268 - accuracy: 0.5655 - val_loss: 1.4222 - val_accuracy: 0.5450\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3735 - accuracy: 0.5869 - val_loss: 1.3721 - val_accuracy: 0.5660\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3220 - accuracy: 0.6060 - val_loss: 1.3231 - val_accuracy: 0.5820\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2723 - accuracy: 0.6280 - val_loss: 1.2785 - val_accuracy: 0.5850\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2254 - accuracy: 0.6368 - val_loss: 1.2328 - val_accuracy: 0.6080\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1808 - accuracy: 0.6508 - val_loss: 1.1920 - val_accuracy: 0.6160\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1387 - accuracy: 0.6628 - val_loss: 1.1529 - val_accuracy: 0.6400\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0999 - accuracy: 0.6729 - val_loss: 1.1176 - val_accuracy: 0.6360\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0633 - accuracy: 0.6791 - val_loss: 1.0839 - val_accuracy: 0.6440\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0290 - accuracy: 0.6868 - val_loss: 1.0530 - val_accuracy: 0.6580\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9972 - accuracy: 0.7000 - val_loss: 1.0257 - val_accuracy: 0.6690\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9679 - accuracy: 0.7053 - val_loss: 0.9973 - val_accuracy: 0.6720\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9402 - accuracy: 0.7099 - val_loss: 0.9767 - val_accuracy: 0.6740\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9156 - accuracy: 0.7145 - val_loss: 0.9531 - val_accuracy: 0.6820\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8923 - accuracy: 0.7224 - val_loss: 0.9289 - val_accuracy: 0.6890\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8705 - accuracy: 0.7267 - val_loss: 0.9088 - val_accuracy: 0.6870\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8501 - accuracy: 0.7331 - val_loss: 0.8910 - val_accuracy: 0.6940\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8310 - accuracy: 0.7363 - val_loss: 0.8787 - val_accuracy: 0.6910\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8137 - accuracy: 0.7404 - val_loss: 0.8656 - val_accuracy: 0.7040\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7972 - accuracy: 0.7456 - val_loss: 0.8507 - val_accuracy: 0.7000\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7824 - accuracy: 0.7495 - val_loss: 0.8375 - val_accuracy: 0.7010\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7675 - accuracy: 0.7528 - val_loss: 0.8261 - val_accuracy: 0.7070\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7545 - accuracy: 0.7576 - val_loss: 0.8151 - val_accuracy: 0.7060\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7412 - accuracy: 0.7593 - val_loss: 0.8064 - val_accuracy: 0.7000\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7294 - accuracy: 0.7665 - val_loss: 0.7946 - val_accuracy: 0.7150\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7178 - accuracy: 0.7671 - val_loss: 0.7841 - val_accuracy: 0.7160\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7064 - accuracy: 0.7711 - val_loss: 0.7779 - val_accuracy: 0.7070\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6964 - accuracy: 0.7747 - val_loss: 0.7683 - val_accuracy: 0.7200\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6864 - accuracy: 0.7801 - val_loss: 0.7660 - val_accuracy: 0.7160\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6773 - accuracy: 0.7799 - val_loss: 0.7553 - val_accuracy: 0.7250\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6683 - accuracy: 0.7825 - val_loss: 0.7502 - val_accuracy: 0.7240\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6595 - accuracy: 0.7847 - val_loss: 0.7439 - val_accuracy: 0.7250\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6513 - accuracy: 0.7887 - val_loss: 0.7346 - val_accuracy: 0.7280\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6434 - accuracy: 0.7916 - val_loss: 0.7310 - val_accuracy: 0.7270\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6356 - accuracy: 0.7919 - val_loss: 0.7237 - val_accuracy: 0.7290\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6279 - accuracy: 0.7965 - val_loss: 0.7204 - val_accuracy: 0.7320\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6207 - accuracy: 0.7997 - val_loss: 0.7179 - val_accuracy: 0.7300\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6137 - accuracy: 0.8012 - val_loss: 0.7167 - val_accuracy: 0.7280\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6063 - accuracy: 0.8051 - val_loss: 0.7128 - val_accuracy: 0.7340\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6003 - accuracy: 0.8064 - val_loss: 0.7101 - val_accuracy: 0.7340\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5937 - accuracy: 0.8087 - val_loss: 0.7019 - val_accuracy: 0.7370\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5871 - accuracy: 0.8099 - val_loss: 0.7008 - val_accuracy: 0.7360\n",
      "Epoch 60/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5816 - accuracy: 0.8105 - val_loss: 0.6936 - val_accuracy: 0.7400\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5761 - accuracy: 0.8120 - val_loss: 0.6936 - val_accuracy: 0.7380\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5697 - accuracy: 0.8160 - val_loss: 0.6918 - val_accuracy: 0.7330\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5639 - accuracy: 0.8168 - val_loss: 0.6876 - val_accuracy: 0.7350\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5589 - accuracy: 0.8173 - val_loss: 0.6861 - val_accuracy: 0.7390\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5533 - accuracy: 0.8215 - val_loss: 0.6862 - val_accuracy: 0.7390\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5481 - accuracy: 0.8221 - val_loss: 0.6814 - val_accuracy: 0.7430\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5431 - accuracy: 0.8255 - val_loss: 0.6798 - val_accuracy: 0.7380\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5381 - accuracy: 0.8256 - val_loss: 0.6771 - val_accuracy: 0.7420\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5329 - accuracy: 0.8285 - val_loss: 0.6742 - val_accuracy: 0.7390\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5279 - accuracy: 0.8293 - val_loss: 0.6736 - val_accuracy: 0.7400\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5235 - accuracy: 0.8315 - val_loss: 0.6712 - val_accuracy: 0.7390\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5187 - accuracy: 0.8321 - val_loss: 0.6701 - val_accuracy: 0.7450\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5144 - accuracy: 0.8349 - val_loss: 0.6684 - val_accuracy: 0.7410\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5100 - accuracy: 0.8359 - val_loss: 0.6677 - val_accuracy: 0.7410\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5053 - accuracy: 0.8383 - val_loss: 0.6645 - val_accuracy: 0.7460\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5009 - accuracy: 0.8415 - val_loss: 0.6604 - val_accuracy: 0.7510\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4966 - accuracy: 0.8408 - val_loss: 0.6652 - val_accuracy: 0.7430\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4924 - accuracy: 0.8444 - val_loss: 0.6626 - val_accuracy: 0.7440\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4886 - accuracy: 0.8443 - val_loss: 0.6633 - val_accuracy: 0.7390\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4845 - accuracy: 0.8453 - val_loss: 0.6592 - val_accuracy: 0.7440\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4809 - accuracy: 0.8465 - val_loss: 0.6568 - val_accuracy: 0.7450\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4765 - accuracy: 0.8467 - val_loss: 0.6552 - val_accuracy: 0.7540\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4722 - accuracy: 0.8513 - val_loss: 0.6555 - val_accuracy: 0.7470\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4683 - accuracy: 0.8511 - val_loss: 0.6588 - val_accuracy: 0.7480\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4647 - accuracy: 0.8533 - val_loss: 0.6554 - val_accuracy: 0.7450\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4605 - accuracy: 0.8541 - val_loss: 0.6547 - val_accuracy: 0.7450\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4568 - accuracy: 0.8555 - val_loss: 0.6554 - val_accuracy: 0.7500\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4538 - accuracy: 0.8557 - val_loss: 0.6508 - val_accuracy: 0.7490\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4496 - accuracy: 0.8557 - val_loss: 0.6512 - val_accuracy: 0.7440\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4459 - accuracy: 0.8579 - val_loss: 0.6535 - val_accuracy: 0.7470\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4428 - accuracy: 0.8596 - val_loss: 0.6491 - val_accuracy: 0.7530\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4392 - accuracy: 0.8601 - val_loss: 0.6509 - val_accuracy: 0.7530\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4358 - accuracy: 0.8612 - val_loss: 0.6479 - val_accuracy: 0.7520\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4326 - accuracy: 0.8621 - val_loss: 0.6487 - val_accuracy: 0.7490\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4289 - accuracy: 0.8636 - val_loss: 0.6487 - val_accuracy: 0.7480\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4254 - accuracy: 0.8653 - val_loss: 0.6449 - val_accuracy: 0.7500\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4225 - accuracy: 0.8659 - val_loss: 0.6539 - val_accuracy: 0.7480\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4188 - accuracy: 0.8679 - val_loss: 0.6501 - val_accuracy: 0.7520\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4159 - accuracy: 0.8696 - val_loss: 0.6503 - val_accuracy: 0.7460\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4125 - accuracy: 0.8697 - val_loss: 0.6486 - val_accuracy: 0.7460\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4094 - accuracy: 0.8712 - val_loss: 0.6517 - val_accuracy: 0.7460\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4064 - accuracy: 0.8720 - val_loss: 0.6463 - val_accuracy: 0.7530\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4029 - accuracy: 0.8748 - val_loss: 0.6466 - val_accuracy: 0.7460\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4002 - accuracy: 0.8744 - val_loss: 0.6492 - val_accuracy: 0.7520\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3967 - accuracy: 0.8744 - val_loss: 0.6480 - val_accuracy: 0.7470\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3934 - accuracy: 0.8776 - val_loss: 0.6508 - val_accuracy: 0.7490\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3908 - accuracy: 0.8784 - val_loss: 0.6463 - val_accuracy: 0.7480\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3878 - accuracy: 0.8800 - val_loss: 0.6483 - val_accuracy: 0.7490\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3848 - accuracy: 0.8805 - val_loss: 0.6494 - val_accuracy: 0.7520\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3819 - accuracy: 0.8824 - val_loss: 0.6505 - val_accuracy: 0.7500\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3794 - accuracy: 0.8800 - val_loss: 0.6485 - val_accuracy: 0.7510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3761 - accuracy: 0.8820 - val_loss: 0.6474 - val_accuracy: 0.7530\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3737 - accuracy: 0.8848 - val_loss: 0.6485 - val_accuracy: 0.7510\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3704 - accuracy: 0.8843 - val_loss: 0.6469 - val_accuracy: 0.7550\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3683 - accuracy: 0.8864 - val_loss: 0.6465 - val_accuracy: 0.7550\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3651 - accuracy: 0.8868 - val_loss: 0.6496 - val_accuracy: 0.7470\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3623 - accuracy: 0.8879 - val_loss: 0.6490 - val_accuracy: 0.7510\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3597 - accuracy: 0.8889 - val_loss: 0.6550 - val_accuracy: 0.7510\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3572 - accuracy: 0.8905 - val_loss: 0.6469 - val_accuracy: 0.7560\n",
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3547 - accuracy: 0.8917 - val_loss: 0.6496 - val_accuracy: 0.7530\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3520 - accuracy: 0.8927 - val_loss: 0.6529 - val_accuracy: 0.7520\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3492 - accuracy: 0.8932 - val_loss: 0.6557 - val_accuracy: 0.7530\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3469 - accuracy: 0.8943 - val_loss: 0.6649 - val_accuracy: 0.7510\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3443 - accuracy: 0.8948 - val_loss: 0.6585 - val_accuracy: 0.7490\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3420 - accuracy: 0.8967 - val_loss: 0.6524 - val_accuracy: 0.7490\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3395 - accuracy: 0.8973 - val_loss: 0.6558 - val_accuracy: 0.7490\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3363 - accuracy: 0.8987 - val_loss: 0.6562 - val_accuracy: 0.7510\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3346 - accuracy: 0.8993 - val_loss: 0.6505 - val_accuracy: 0.7500\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3319 - accuracy: 0.8999 - val_loss: 0.6519 - val_accuracy: 0.7540\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3293 - accuracy: 0.9009 - val_loss: 0.6537 - val_accuracy: 0.7610\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3275 - accuracy: 0.9032 - val_loss: 0.6533 - val_accuracy: 0.7570\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3244 - accuracy: 0.9047 - val_loss: 0.6596 - val_accuracy: 0.7540\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3224 - accuracy: 0.9056 - val_loss: 0.6519 - val_accuracy: 0.7590\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3199 - accuracy: 0.9049 - val_loss: 0.6618 - val_accuracy: 0.7510\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3179 - accuracy: 0.9059 - val_loss: 0.6521 - val_accuracy: 0.7580\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3154 - accuracy: 0.9084 - val_loss: 0.6622 - val_accuracy: 0.7540\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3131 - accuracy: 0.9088 - val_loss: 0.6600 - val_accuracy: 0.7550\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3113 - accuracy: 0.9089 - val_loss: 0.6595 - val_accuracy: 0.7550\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3086 - accuracy: 0.9115 - val_loss: 0.6621 - val_accuracy: 0.7600\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3066 - accuracy: 0.9116 - val_loss: 0.6644 - val_accuracy: 0.7590\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3045 - accuracy: 0.9107 - val_loss: 0.6682 - val_accuracy: 0.7560\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3021 - accuracy: 0.9103 - val_loss: 0.6657 - val_accuracy: 0.7560\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2999 - accuracy: 0.9137 - val_loss: 0.6667 - val_accuracy: 0.7530\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2977 - accuracy: 0.9153 - val_loss: 0.6624 - val_accuracy: 0.7580\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2956 - accuracy: 0.9156 - val_loss: 0.6699 - val_accuracy: 0.7550\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.2936 - accuracy: 0.9171 - val_loss: 0.6737 - val_accuracy: 0.7540\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2915 - accuracy: 0.9167 - val_loss: 0.6635 - val_accuracy: 0.7590\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2892 - accuracy: 0.9179 - val_loss: 0.6711 - val_accuracy: 0.7550\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.2871 - accuracy: 0.9188 - val_loss: 0.6734 - val_accuracy: 0.7580\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.2853 - accuracy: 0.9208 - val_loss: 0.6710 - val_accuracy: 0.7570\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "baseline_model_val = baseline_model.fit(X_train_tokens, \n",
    "                                       y_train_lb,\n",
    "                                       epochs=150,\n",
    "                                       batch_size=256,\n",
    "                                       validation_data=(X_val_tokens,\n",
    "                                                       y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance\n",
    "\n",
    "The attribute `.history` (stored as a dictionary) contains four entries now: one per metric that was being monitored during training and validation. Print the keys of this dictionary for confirmation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:29.136319Z",
     "start_time": "2020-05-12T14:46:29.129504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the history attribute and store the dictionary\n",
    "baseline_model_val_dict = baseline_model_val.history\n",
    "\n",
    "# Print the keys\n",
    "baseline_model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:29.317967Z",
     "start_time": "2020-05-12T14:46:29.138321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step\n",
      "----------\n",
      "Training Loss: 0.283 \n",
      "Training Accuracy: 0.919\n"
     ]
    }
   ],
   "source": [
    "results_train = baseline_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print('----------')\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate this model on the test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:29.361968Z",
     "start_time": "2020-05-12T14:46:29.318967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 23us/step\n",
      "----------\n",
      "Test Loss: 0.623 \n",
      "Test Accuracy: 0.777\n"
     ]
    }
   ],
   "source": [
    "results_test = baseline_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print('----------')\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results \n",
    "\n",
    "Plot the loss versus the number of epochs. Be sure to include the training and the validation loss in the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:29.602782Z",
     "start_time": "2020-05-12T14:46:29.362968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19c8be7c0b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "loss_values = baseline_model_val_dict['loss']\n",
    "val_loss_values = baseline_model_val_dict['val_loss'] \n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "ax.plot(epochs, loss_values, label= 'Training loss')\n",
    "ax.plot(epochs, val_loss_values, label= 'Validation loss')\n",
    "ax.set_title('Training & validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second plot comparing training and validation accuracy to the number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.692464Z",
     "start_time": "2020-05-12T14:46:29.603784Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8ffc4d6c53f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Accuracy vs number of epochs with train and validation sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model_val_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training acc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Accuracy vs number of epochs with train and validation sets\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "acc_values = baseline_model_val_dict['acc'] \n",
    "val_acc_values = baseline_model_val_dict['val_acc']\n",
    "ax.plot(epochs, acc_values, label='Training acc')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc')\n",
    "ax.set_title('Training & validation accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice an interesting pattern here? Although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss don't necessarily do the same. After a certain point, validation accuracy keeps swinging, which means that you're probably **overfitting** the model to the training data when you train for many epochs past a certain dropoff point. Let's tackle this now. You will now specify an early stopping point when training your model. \n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "Overfitting neural networks is something you **_want_** to avoid at all costs. However, it's not possible to know in advance how many *epochs* you need to train your model on, and running the model multiple times with varying number of *epochs* maybe helpful, but is a time-consuming process. \n",
    "\n",
    "We've defined a model with the same architecture as above. This time specify an early stopping point when training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.694464Z",
     "start_time": "2020-05-12T14:45:46.114Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model_2.add(layers.Dense(25, activation='relu'))\n",
    "model_2.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer='SGD', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import `EarlyStopping` and `ModelCheckpoint` from `keras.callbacks` \n",
    "- Define a list, `early_stopping`: \n",
    "  - Monitor `'val_loss'` and continue training for 10 epochs before stopping \n",
    "  - Save the best model while monitoring `'val_loss'` \n",
    " \n",
    "> If you need help, consult [documentation](https://keras.io/callbacks/).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.695464Z",
     "start_time": "2020-05-12T14:45:46.146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import EarlyStopping and ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define the callbacks\n",
    "early_stopping = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "                 ModelCheckpoint(filepath='best_model.h5', monitor='val_loss',\n",
    "                                save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train `model_2`. Make sure you set the `callbacks` argument to `early_stopping`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.696464Z",
     "start_time": "2020-05-12T14:45:46.179Z"
    }
   },
   "outputs": [],
   "source": [
    "model_2_val = model_2.fit(X_train_tokens, \n",
    "                          y_train_lb, \n",
    "                          epochs=150, \n",
    "                          callbacks=early_stopping, \n",
    "                          batch_size=256, \n",
    "                          validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best (saved) model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.697465Z",
     "start_time": "2020-05-12T14:45:46.213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the best (saved) model\n",
    "from keras.models import load_model\n",
    "saved_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this model to to calculate the training and test accuracy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:46:30.699297Z",
     "start_time": "2020-05-12T14:45:46.246Z"
    }
   },
   "outputs": [],
   "source": [
    "results_train = saved_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = saved_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! Did you notice that the model didn't train for all 150 epochs? You reduced your training time. \n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance. \n",
    "\n",
    "## L2 Regularization \n",
    "\n",
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform. \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L2 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:24.464273Z",
     "start_time": "2020-05-12T14:47:53.748078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.5951 - accuracy: 0.1751 - val_loss: 2.5818 - val_accuracy: 0.2020\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.5638 - accuracy: 0.2068 - val_loss: 2.5568 - val_accuracy: 0.2130\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.5352 - accuracy: 0.2348 - val_loss: 2.5308 - val_accuracy: 0.2410\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.5050 - accuracy: 0.2599 - val_loss: 2.5019 - val_accuracy: 0.2630\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.4712 - accuracy: 0.2871 - val_loss: 2.4691 - val_accuracy: 0.2800\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.4336 - accuracy: 0.3149 - val_loss: 2.4314 - val_accuracy: 0.3050\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3933 - accuracy: 0.3407 - val_loss: 2.3925 - val_accuracy: 0.3200\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.3509 - accuracy: 0.3615 - val_loss: 2.3494 - val_accuracy: 0.3530\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3074 - accuracy: 0.3923 - val_loss: 2.3072 - val_accuracy: 0.3780\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.2638 - accuracy: 0.4153 - val_loss: 2.2650 - val_accuracy: 0.3870\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.2200 - accuracy: 0.4371 - val_loss: 2.2218 - val_accuracy: 0.4090\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.1765 - accuracy: 0.4589 - val_loss: 2.1784 - val_accuracy: 0.4250\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.1330 - accuracy: 0.4812 - val_loss: 2.1352 - val_accuracy: 0.4540\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.0898 - accuracy: 0.5049 - val_loss: 2.0938 - val_accuracy: 0.4760\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0470 - accuracy: 0.5247 - val_loss: 2.0489 - val_accuracy: 0.4980\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.0039 - accuracy: 0.5448 - val_loss: 2.0044 - val_accuracy: 0.5310\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9615 - accuracy: 0.5651 - val_loss: 1.9625 - val_accuracy: 0.5480\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9198 - accuracy: 0.5847 - val_loss: 1.9230 - val_accuracy: 0.5460\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8790 - accuracy: 0.5995 - val_loss: 1.8832 - val_accuracy: 0.5740\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8397 - accuracy: 0.6137 - val_loss: 1.8427 - val_accuracy: 0.5980\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8014 - accuracy: 0.6327 - val_loss: 1.8072 - val_accuracy: 0.6010\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7648 - accuracy: 0.6437 - val_loss: 1.7676 - val_accuracy: 0.6370\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7297 - accuracy: 0.6569 - val_loss: 1.7371 - val_accuracy: 0.6320\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6959 - accuracy: 0.6643 - val_loss: 1.7076 - val_accuracy: 0.6370\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6645 - accuracy: 0.6715 - val_loss: 1.6745 - val_accuracy: 0.6500\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6344 - accuracy: 0.6812 - val_loss: 1.6483 - val_accuracy: 0.6460\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6059 - accuracy: 0.6880 - val_loss: 1.6222 - val_accuracy: 0.6660\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5790 - accuracy: 0.6940 - val_loss: 1.5937 - val_accuracy: 0.6700\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5533 - accuracy: 0.7012 - val_loss: 1.5694 - val_accuracy: 0.6790\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5293 - accuracy: 0.7065 - val_loss: 1.5508 - val_accuracy: 0.6750\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5068 - accuracy: 0.7104 - val_loss: 1.5281 - val_accuracy: 0.6860\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4848 - accuracy: 0.7196 - val_loss: 1.5086 - val_accuracy: 0.6860\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4649 - accuracy: 0.7211 - val_loss: 1.4921 - val_accuracy: 0.6900\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4459 - accuracy: 0.7248 - val_loss: 1.4755 - val_accuracy: 0.6920\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4281 - accuracy: 0.7308 - val_loss: 1.4588 - val_accuracy: 0.6960\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4111 - accuracy: 0.7329 - val_loss: 1.4427 - val_accuracy: 0.7010\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3949 - accuracy: 0.7373 - val_loss: 1.4287 - val_accuracy: 0.7140\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3796 - accuracy: 0.7411 - val_loss: 1.4152 - val_accuracy: 0.7110\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3645 - accuracy: 0.7451 - val_loss: 1.4069 - val_accuracy: 0.7040\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3514 - accuracy: 0.7492 - val_loss: 1.3922 - val_accuracy: 0.7090\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3378 - accuracy: 0.7511 - val_loss: 1.3795 - val_accuracy: 0.7120\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3253 - accuracy: 0.7547 - val_loss: 1.3686 - val_accuracy: 0.7210\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3128 - accuracy: 0.7575 - val_loss: 1.3606 - val_accuracy: 0.7110\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3011 - accuracy: 0.7627 - val_loss: 1.3487 - val_accuracy: 0.7240\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2900 - accuracy: 0.7643 - val_loss: 1.3418 - val_accuracy: 0.7250\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2795 - accuracy: 0.7663 - val_loss: 1.3318 - val_accuracy: 0.7190\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2686 - accuracy: 0.7651 - val_loss: 1.3228 - val_accuracy: 0.7250\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2587 - accuracy: 0.7744 - val_loss: 1.3132 - val_accuracy: 0.7270\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2490 - accuracy: 0.7749 - val_loss: 1.3120 - val_accuracy: 0.7250\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2396 - accuracy: 0.7776 - val_loss: 1.3010 - val_accuracy: 0.7270\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2305 - accuracy: 0.7803 - val_loss: 1.2927 - val_accuracy: 0.7300\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2216 - accuracy: 0.7852 - val_loss: 1.2896 - val_accuracy: 0.7320\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2130 - accuracy: 0.7837 - val_loss: 1.2788 - val_accuracy: 0.7260\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2045 - accuracy: 0.7911 - val_loss: 1.2760 - val_accuracy: 0.7240\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1965 - accuracy: 0.7907 - val_loss: 1.2708 - val_accuracy: 0.7270\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1886 - accuracy: 0.7919 - val_loss: 1.2587 - val_accuracy: 0.7350\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1807 - accuracy: 0.7951 - val_loss: 1.2583 - val_accuracy: 0.7340\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1729 - accuracy: 0.7937 - val_loss: 1.2507 - val_accuracy: 0.7320\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1653 - accuracy: 0.7968 - val_loss: 1.2477 - val_accuracy: 0.7350\n",
      "Epoch 60/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1580 - accuracy: 0.7988 - val_loss: 1.2405 - val_accuracy: 0.7330\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1509 - accuracy: 0.8023 - val_loss: 1.2351 - val_accuracy: 0.7360\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1437 - accuracy: 0.8021 - val_loss: 1.2313 - val_accuracy: 0.7420\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1374 - accuracy: 0.8037 - val_loss: 1.2259 - val_accuracy: 0.7370\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1301 - accuracy: 0.8084 - val_loss: 1.2228 - val_accuracy: 0.7380\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1234 - accuracy: 0.8088 - val_loss: 1.2187 - val_accuracy: 0.7400\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1171 - accuracy: 0.8093 - val_loss: 1.2126 - val_accuracy: 0.7400\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1105 - accuracy: 0.8117 - val_loss: 1.2069 - val_accuracy: 0.7360\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1041 - accuracy: 0.8155 - val_loss: 1.2027 - val_accuracy: 0.7420\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0983 - accuracy: 0.8160 - val_loss: 1.1973 - val_accuracy: 0.7450\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0923 - accuracy: 0.8163 - val_loss: 1.1923 - val_accuracy: 0.7380\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0865 - accuracy: 0.8201 - val_loss: 1.1911 - val_accuracy: 0.7400\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0802 - accuracy: 0.8197 - val_loss: 1.1857 - val_accuracy: 0.7430\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0746 - accuracy: 0.8217 - val_loss: 1.1866 - val_accuracy: 0.7390\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0686 - accuracy: 0.8240 - val_loss: 1.1806 - val_accuracy: 0.7400\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0631 - accuracy: 0.8251 - val_loss: 1.1750 - val_accuracy: 0.7440\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0568 - accuracy: 0.8273 - val_loss: 1.1787 - val_accuracy: 0.7400\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0517 - accuracy: 0.8271 - val_loss: 1.1729 - val_accuracy: 0.7360\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0459 - accuracy: 0.8291 - val_loss: 1.1678 - val_accuracy: 0.7370\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0403 - accuracy: 0.8317 - val_loss: 1.1685 - val_accuracy: 0.7400\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0353 - accuracy: 0.8319 - val_loss: 1.1623 - val_accuracy: 0.7420\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0300 - accuracy: 0.8339 - val_loss: 1.1600 - val_accuracy: 0.7420\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0246 - accuracy: 0.8340 - val_loss: 1.1531 - val_accuracy: 0.7450\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0196 - accuracy: 0.8363 - val_loss: 1.1513 - val_accuracy: 0.7390\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0140 - accuracy: 0.8384 - val_loss: 1.1460 - val_accuracy: 0.7490\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0093 - accuracy: 0.8372 - val_loss: 1.1455 - val_accuracy: 0.7430\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0045 - accuracy: 0.8411 - val_loss: 1.1416 - val_accuracy: 0.7450\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9994 - accuracy: 0.8393 - val_loss: 1.1483 - val_accuracy: 0.7420\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9948 - accuracy: 0.8408 - val_loss: 1.1368 - val_accuracy: 0.7520\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9895 - accuracy: 0.8455 - val_loss: 1.1411 - val_accuracy: 0.7430\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9849 - accuracy: 0.8455 - val_loss: 1.1332 - val_accuracy: 0.7410\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9800 - accuracy: 0.8469 - val_loss: 1.1353 - val_accuracy: 0.7460\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9756 - accuracy: 0.8457 - val_loss: 1.1268 - val_accuracy: 0.7530\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9709 - accuracy: 0.8475 - val_loss: 1.1212 - val_accuracy: 0.7530\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9660 - accuracy: 0.8489 - val_loss: 1.1213 - val_accuracy: 0.7510\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9614 - accuracy: 0.8487 - val_loss: 1.1193 - val_accuracy: 0.7480\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9572 - accuracy: 0.8501 - val_loss: 1.1190 - val_accuracy: 0.7440\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9527 - accuracy: 0.8535 - val_loss: 1.1135 - val_accuracy: 0.7420\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9480 - accuracy: 0.8555 - val_loss: 1.1105 - val_accuracy: 0.7470\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9441 - accuracy: 0.8556 - val_loss: 1.1104 - val_accuracy: 0.7460\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9391 - accuracy: 0.8564 - val_loss: 1.1108 - val_accuracy: 0.7500\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9351 - accuracy: 0.8579 - val_loss: 1.1069 - val_accuracy: 0.7490\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9308 - accuracy: 0.8604 - val_loss: 1.1039 - val_accuracy: 0.7440\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9269 - accuracy: 0.8600 - val_loss: 1.1023 - val_accuracy: 0.7520\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9226 - accuracy: 0.8615 - val_loss: 1.1021 - val_accuracy: 0.7430\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9186 - accuracy: 0.8621 - val_loss: 1.0949 - val_accuracy: 0.7540\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9143 - accuracy: 0.8629 - val_loss: 1.0958 - val_accuracy: 0.7470\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9105 - accuracy: 0.8640 - val_loss: 1.0905 - val_accuracy: 0.7520\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9066 - accuracy: 0.8655 - val_loss: 1.0942 - val_accuracy: 0.7470\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9021 - accuracy: 0.8667 - val_loss: 1.0874 - val_accuracy: 0.7540\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8985 - accuracy: 0.8676 - val_loss: 1.0849 - val_accuracy: 0.7520\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8943 - accuracy: 0.8687 - val_loss: 1.0870 - val_accuracy: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8906 - accuracy: 0.8685 - val_loss: 1.0868 - val_accuracy: 0.7470\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8866 - accuracy: 0.8708 - val_loss: 1.0804 - val_accuracy: 0.7520\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8826 - accuracy: 0.8700 - val_loss: 1.0797 - val_accuracy: 0.7530\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8789 - accuracy: 0.8719 - val_loss: 1.0839 - val_accuracy: 0.7470\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8749 - accuracy: 0.8733 - val_loss: 1.0787 - val_accuracy: 0.7450\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8713 - accuracy: 0.8748 - val_loss: 1.0779 - val_accuracy: 0.7550\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8679 - accuracy: 0.8735 - val_loss: 1.0719 - val_accuracy: 0.7490\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8641 - accuracy: 0.8763 - val_loss: 1.0736 - val_accuracy: 0.7470\n",
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8604 - accuracy: 0.8776 - val_loss: 1.0739 - val_accuracy: 0.7530\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8569 - accuracy: 0.8759 - val_loss: 1.0692 - val_accuracy: 0.7510\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8530 - accuracy: 0.8776 - val_loss: 1.0652 - val_accuracy: 0.7580\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8496 - accuracy: 0.8781 - val_loss: 1.0673 - val_accuracy: 0.7510\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8463 - accuracy: 0.8813 - val_loss: 1.0741 - val_accuracy: 0.7510\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8427 - accuracy: 0.8812 - val_loss: 1.0629 - val_accuracy: 0.7500\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8390 - accuracy: 0.8824 - val_loss: 1.0612 - val_accuracy: 0.7520\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8355 - accuracy: 0.8832 - val_loss: 1.0620 - val_accuracy: 0.7510\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8325 - accuracy: 0.8831 - val_loss: 1.0556 - val_accuracy: 0.7560\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8286 - accuracy: 0.8827 - val_loss: 1.0566 - val_accuracy: 0.7560\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8259 - accuracy: 0.8851 - val_loss: 1.0521 - val_accuracy: 0.7570\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8221 - accuracy: 0.8859 - val_loss: 1.0530 - val_accuracy: 0.7540\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8192 - accuracy: 0.8864 - val_loss: 1.0534 - val_accuracy: 0.7540\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8157 - accuracy: 0.8872 - val_loss: 1.0502 - val_accuracy: 0.7590\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8124 - accuracy: 0.8880 - val_loss: 1.0509 - val_accuracy: 0.7560\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - accuracy: 0.8883 - val_loss: 1.0462 - val_accuracy: 0.7550\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8060 - accuracy: 0.8911 - val_loss: 1.0467 - val_accuracy: 0.7550\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8028 - accuracy: 0.8919 - val_loss: 1.0440 - val_accuracy: 0.7540\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7998 - accuracy: 0.8923 - val_loss: 1.0483 - val_accuracy: 0.7550\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7968 - accuracy: 0.8916 - val_loss: 1.0433 - val_accuracy: 0.7560\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7937 - accuracy: 0.8925 - val_loss: 1.0440 - val_accuracy: 0.7570\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7904 - accuracy: 0.8941 - val_loss: 1.0436 - val_accuracy: 0.7560\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7871 - accuracy: 0.8941 - val_loss: 1.0450 - val_accuracy: 0.7570\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7842 - accuracy: 0.8951 - val_loss: 1.0416 - val_accuracy: 0.7570\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7812 - accuracy: 0.8959 - val_loss: 1.0379 - val_accuracy: 0.7590\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7783 - accuracy: 0.8979 - val_loss: 1.0380 - val_accuracy: 0.7590\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7756 - accuracy: 0.8988 - val_loss: 1.0420 - val_accuracy: 0.7580\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7724 - accuracy: 0.8985 - val_loss: 1.0365 - val_accuracy: 0.7610\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7699 - accuracy: 0.8991 - val_loss: 1.0375 - val_accuracy: 0.7580\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7665 - accuracy: 0.9000 - val_loss: 1.0384 - val_accuracy: 0.7600\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7636 - accuracy: 0.9005 - val_loss: 1.0301 - val_accuracy: 0.7590\n"
     ]
    }
   ],
   "source": [
    "# Import regularizers\n",
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "L2_model = models.Sequential()\n",
    "# Add the input and first hidden layer\n",
    "L2_model.add(layers.Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,)))\n",
    "# Add another hidden layer\n",
    "L2_model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "# Add an output layer\n",
    "L2_model.add(layers.Dense(7, activation='softmax'))\n",
    "# Compile the model\n",
    "L2_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "# Train the model \n",
    "L2_model_val = L2_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training as well as the validation accuracy for both the L2 and the baseline models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:48:58.247648Z",
     "start_time": "2020-05-12T14:48:58.228677Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-678c770a1083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# L2 model details\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mL2_model_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mL2_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mL2_val_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Baseline model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "# L2 model details\n",
    "L2_model_dict = L2_model_val.history\n",
    "L2_acc_values = L2_model_dict['acc'] \n",
    "L2_val_acc_values = L2_model_dict['val_acc']\n",
    "# Baseline model\n",
    "baseline_model_acc = baseline_model_val_dict['acc'] \n",
    "baseline_model_val_acc = baseline_model_val_dict['val_acc']\n",
    "# Plot the accuracy for these models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, L2_acc_values, label='Training acc (L2)')\n",
    "ax.plot(epochs, L2_val_acc_values, label='Validation acc (L2)')\n",
    "ax.plot(epochs, baseline_model_acc, label='Training acc (Baseline)')\n",
    "ax.plot(epochs, baseline_model_val_acc, label='Validation acc (Baseline)')\n",
    "ax.set_title('Training & validation accuracy L2 vs regular')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better.  \n",
    "\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "Now have a look at L1 regularization. Will this work better? \n",
    "\n",
    "- Use 2 hidden layers with 50 units in the first and 25 in the second layer, both with `'relu'` activation functions \n",
    "- Add L1 regularization to both the hidden layers with 0.005 as the `lambda_coeff` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:52:20.247234Z",
     "start_time": "2020-05-12T14:51:50.772923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 15.9586 - accuracy: 0.1680 - val_loss: 15.5623 - val_accuracy: 0.1540\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 15.2000 - accuracy: 0.1937 - val_loss: 14.8196 - val_accuracy: 0.1780\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 14.4652 - accuracy: 0.2165 - val_loss: 14.0987 - val_accuracy: 0.2010\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 13.7509 - accuracy: 0.2388 - val_loss: 13.3967 - val_accuracy: 0.2140\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 13.0568 - accuracy: 0.2601 - val_loss: 12.7148 - val_accuracy: 0.2450\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 12.3836 - accuracy: 0.2857 - val_loss: 12.0536 - val_accuracy: 0.2750\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 11.7302 - accuracy: 0.3097 - val_loss: 11.4124 - val_accuracy: 0.3010\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 11.0967 - accuracy: 0.3357 - val_loss: 10.7915 - val_accuracy: 0.3270\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 10.4840 - accuracy: 0.3585 - val_loss: 10.1892 - val_accuracy: 0.3630\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 9.8914 - accuracy: 0.3939 - val_loss: 9.6085 - val_accuracy: 0.3890\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 9.3195 - accuracy: 0.4188 - val_loss: 9.0487 - val_accuracy: 0.4040\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 8.7694 - accuracy: 0.4379 - val_loss: 8.5100 - val_accuracy: 0.4320\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 8.2410 - accuracy: 0.4681 - val_loss: 7.9931 - val_accuracy: 0.4510\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 7.7346 - accuracy: 0.4945 - val_loss: 7.5007 - val_accuracy: 0.4790\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 7.2499 - accuracy: 0.5251 - val_loss: 7.0265 - val_accuracy: 0.5190\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.7870 - accuracy: 0.5480 - val_loss: 6.5771 - val_accuracy: 0.5390\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.3474 - accuracy: 0.5644 - val_loss: 6.1509 - val_accuracy: 0.5650\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 5.9305 - accuracy: 0.5816 - val_loss: 5.7452 - val_accuracy: 0.5790\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 5.5357 - accuracy: 0.5975 - val_loss: 5.3620 - val_accuracy: 0.5860\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 5.1630 - accuracy: 0.6087 - val_loss: 5.0000 - val_accuracy: 0.6020\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 4.8127 - accuracy: 0.6192 - val_loss: 4.6634 - val_accuracy: 0.6010\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.4851 - accuracy: 0.6307 - val_loss: 4.3465 - val_accuracy: 0.6230\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.1800 - accuracy: 0.6417 - val_loss: 4.0541 - val_accuracy: 0.6240\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.8971 - accuracy: 0.6464 - val_loss: 3.7835 - val_accuracy: 0.6370\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.6351 - accuracy: 0.6540 - val_loss: 3.5312 - val_accuracy: 0.6410\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.3958 - accuracy: 0.6588 - val_loss: 3.3026 - val_accuracy: 0.6420\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.1778 - accuracy: 0.6609 - val_loss: 3.0963 - val_accuracy: 0.6510\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.9817 - accuracy: 0.6685 - val_loss: 2.9135 - val_accuracy: 0.6480\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.8079 - accuracy: 0.6659 - val_loss: 2.7480 - val_accuracy: 0.6530\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.6536 - accuracy: 0.6705 - val_loss: 2.6066 - val_accuracy: 0.6720\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.5209 - accuracy: 0.6749 - val_loss: 2.4840 - val_accuracy: 0.6700\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.4076 - accuracy: 0.6761 - val_loss: 2.3770 - val_accuracy: 0.6610\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3123 - accuracy: 0.6775 - val_loss: 2.2905 - val_accuracy: 0.6620\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.2346 - accuracy: 0.6788 - val_loss: 2.2221 - val_accuracy: 0.6710\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.1745 - accuracy: 0.6785 - val_loss: 2.1670 - val_accuracy: 0.6730\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.1277 - accuracy: 0.6809 - val_loss: 2.1296 - val_accuracy: 0.6710\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0921 - accuracy: 0.6807 - val_loss: 2.0962 - val_accuracy: 0.6750\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0626 - accuracy: 0.6848 - val_loss: 2.0683 - val_accuracy: 0.6700\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0365 - accuracy: 0.6839 - val_loss: 2.0457 - val_accuracy: 0.6740\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0133 - accuracy: 0.6848 - val_loss: 2.0184 - val_accuracy: 0.6730\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9907 - accuracy: 0.6872 - val_loss: 1.9985 - val_accuracy: 0.6810\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9698 - accuracy: 0.6872 - val_loss: 1.9777 - val_accuracy: 0.6790\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9500 - accuracy: 0.6903 - val_loss: 1.9586 - val_accuracy: 0.6790\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9310 - accuracy: 0.6905 - val_loss: 1.9398 - val_accuracy: 0.6860\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9132 - accuracy: 0.6887 - val_loss: 1.9236 - val_accuracy: 0.6740\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8952 - accuracy: 0.6908 - val_loss: 1.9026 - val_accuracy: 0.6830\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8786 - accuracy: 0.6919 - val_loss: 1.8880 - val_accuracy: 0.6810\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8622 - accuracy: 0.6921 - val_loss: 1.8722 - val_accuracy: 0.6820\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8472 - accuracy: 0.6944 - val_loss: 1.8592 - val_accuracy: 0.6890\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8324 - accuracy: 0.6953 - val_loss: 1.8410 - val_accuracy: 0.6850\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8181 - accuracy: 0.6959 - val_loss: 1.8263 - val_accuracy: 0.6870\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8039 - accuracy: 0.6967 - val_loss: 1.8151 - val_accuracy: 0.6860\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7909 - accuracy: 0.6964 - val_loss: 1.8032 - val_accuracy: 0.6890\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7776 - accuracy: 0.6967 - val_loss: 1.7878 - val_accuracy: 0.6880\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7656 - accuracy: 0.6985 - val_loss: 1.7738 - val_accuracy: 0.6880\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7530 - accuracy: 0.6993 - val_loss: 1.7646 - val_accuracy: 0.6880\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7418 - accuracy: 0.6987 - val_loss: 1.7503 - val_accuracy: 0.6890\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7302 - accuracy: 0.6993 - val_loss: 1.7382 - val_accuracy: 0.6890\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7195 - accuracy: 0.6988 - val_loss: 1.7313 - val_accuracy: 0.6950\n",
      "Epoch 60/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7088 - accuracy: 0.6983 - val_loss: 1.7182 - val_accuracy: 0.6950\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6983 - accuracy: 0.7007 - val_loss: 1.7114 - val_accuracy: 0.6920\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6887 - accuracy: 0.7020 - val_loss: 1.7007 - val_accuracy: 0.6930\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6784 - accuracy: 0.7011 - val_loss: 1.6886 - val_accuracy: 0.6970\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6689 - accuracy: 0.7039 - val_loss: 1.6819 - val_accuracy: 0.6940\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6599 - accuracy: 0.7027 - val_loss: 1.6704 - val_accuracy: 0.6930\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6509 - accuracy: 0.7029 - val_loss: 1.6620 - val_accuracy: 0.6950\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6420 - accuracy: 0.7021 - val_loss: 1.6526 - val_accuracy: 0.6960\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6328 - accuracy: 0.7047 - val_loss: 1.6441 - val_accuracy: 0.6980\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6250 - accuracy: 0.7028 - val_loss: 1.6349 - val_accuracy: 0.6960\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6163 - accuracy: 0.7047 - val_loss: 1.6266 - val_accuracy: 0.6950\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6082 - accuracy: 0.7044 - val_loss: 1.6193 - val_accuracy: 0.6910\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6001 - accuracy: 0.7051 - val_loss: 1.6091 - val_accuracy: 0.6930\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5919 - accuracy: 0.7068 - val_loss: 1.6027 - val_accuracy: 0.6950\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5842 - accuracy: 0.7085 - val_loss: 1.5956 - val_accuracy: 0.6960\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5765 - accuracy: 0.7080 - val_loss: 1.5896 - val_accuracy: 0.6940\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5695 - accuracy: 0.7095 - val_loss: 1.5796 - val_accuracy: 0.6930\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5620 - accuracy: 0.7095 - val_loss: 1.5750 - val_accuracy: 0.6930\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5551 - accuracy: 0.7099 - val_loss: 1.5702 - val_accuracy: 0.6980\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5482 - accuracy: 0.7081 - val_loss: 1.5602 - val_accuracy: 0.7010\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5410 - accuracy: 0.7096 - val_loss: 1.5540 - val_accuracy: 0.6970\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5341 - accuracy: 0.7100 - val_loss: 1.5463 - val_accuracy: 0.7030\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5273 - accuracy: 0.7108 - val_loss: 1.5413 - val_accuracy: 0.6980\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5203 - accuracy: 0.7108 - val_loss: 1.5341 - val_accuracy: 0.6970\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5137 - accuracy: 0.7081 - val_loss: 1.5248 - val_accuracy: 0.6950\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5070 - accuracy: 0.7107 - val_loss: 1.5216 - val_accuracy: 0.6970\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5007 - accuracy: 0.7109 - val_loss: 1.5118 - val_accuracy: 0.6960\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4940 - accuracy: 0.7127 - val_loss: 1.5052 - val_accuracy: 0.6980\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4872 - accuracy: 0.7124 - val_loss: 1.5017 - val_accuracy: 0.6940\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4816 - accuracy: 0.7111 - val_loss: 1.4944 - val_accuracy: 0.7050\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4753 - accuracy: 0.7125 - val_loss: 1.4872 - val_accuracy: 0.7040\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4691 - accuracy: 0.7141 - val_loss: 1.4835 - val_accuracy: 0.7000\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4632 - accuracy: 0.7131 - val_loss: 1.4775 - val_accuracy: 0.7000\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4570 - accuracy: 0.7141 - val_loss: 1.4680 - val_accuracy: 0.7000\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4509 - accuracy: 0.7127 - val_loss: 1.4647 - val_accuracy: 0.7000\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4456 - accuracy: 0.7129 - val_loss: 1.4593 - val_accuracy: 0.6950\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4400 - accuracy: 0.7152 - val_loss: 1.4534 - val_accuracy: 0.7020\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4342 - accuracy: 0.7135 - val_loss: 1.4510 - val_accuracy: 0.6970\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4286 - accuracy: 0.7140 - val_loss: 1.4431 - val_accuracy: 0.7040\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4228 - accuracy: 0.7159 - val_loss: 1.4398 - val_accuracy: 0.7050\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4176 - accuracy: 0.7147 - val_loss: 1.4308 - val_accuracy: 0.7000\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4121 - accuracy: 0.7156 - val_loss: 1.4259 - val_accuracy: 0.7040\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4068 - accuracy: 0.7159 - val_loss: 1.4249 - val_accuracy: 0.6990\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4015 - accuracy: 0.7169 - val_loss: 1.4250 - val_accuracy: 0.7060\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3969 - accuracy: 0.7173 - val_loss: 1.4105 - val_accuracy: 0.7000\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3912 - accuracy: 0.7176 - val_loss: 1.4075 - val_accuracy: 0.7030\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3858 - accuracy: 0.7179 - val_loss: 1.4028 - val_accuracy: 0.7010\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3809 - accuracy: 0.7167 - val_loss: 1.3967 - val_accuracy: 0.7010\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3754 - accuracy: 0.7176 - val_loss: 1.3911 - val_accuracy: 0.7050\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3713 - accuracy: 0.7185 - val_loss: 1.3884 - val_accuracy: 0.7030\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3661 - accuracy: 0.7189 - val_loss: 1.3829 - val_accuracy: 0.7010\n",
      "Epoch 111/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3614 - accuracy: 0.7189 - val_loss: 1.3782 - val_accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3562 - accuracy: 0.7188 - val_loss: 1.3739 - val_accuracy: 0.7050\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3515 - accuracy: 0.7200 - val_loss: 1.3663 - val_accuracy: 0.7030\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3466 - accuracy: 0.7191 - val_loss: 1.3659 - val_accuracy: 0.7020\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3428 - accuracy: 0.7195 - val_loss: 1.3636 - val_accuracy: 0.6970\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3378 - accuracy: 0.7196 - val_loss: 1.3564 - val_accuracy: 0.7020\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3334 - accuracy: 0.7203 - val_loss: 1.3517 - val_accuracy: 0.7020\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3292 - accuracy: 0.7200 - val_loss: 1.3480 - val_accuracy: 0.6990\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3244 - accuracy: 0.7216 - val_loss: 1.3450 - val_accuracy: 0.7100\n",
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3206 - accuracy: 0.7207 - val_loss: 1.3423 - val_accuracy: 0.7020\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3160 - accuracy: 0.7217 - val_loss: 1.3338 - val_accuracy: 0.7070\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3120 - accuracy: 0.7213 - val_loss: 1.3326 - val_accuracy: 0.7050\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3083 - accuracy: 0.7228 - val_loss: 1.3263 - val_accuracy: 0.7010\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3037 - accuracy: 0.7225 - val_loss: 1.3211 - val_accuracy: 0.7070\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2999 - accuracy: 0.7241 - val_loss: 1.3180 - val_accuracy: 0.7080\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2953 - accuracy: 0.7240 - val_loss: 1.3129 - val_accuracy: 0.7120\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2914 - accuracy: 0.7233 - val_loss: 1.3164 - val_accuracy: 0.6980\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2876 - accuracy: 0.7243 - val_loss: 1.3074 - val_accuracy: 0.7040\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2840 - accuracy: 0.7243 - val_loss: 1.3075 - val_accuracy: 0.7080\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2802 - accuracy: 0.7251 - val_loss: 1.2999 - val_accuracy: 0.7080\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2762 - accuracy: 0.7243 - val_loss: 1.2970 - val_accuracy: 0.7070\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2730 - accuracy: 0.7237 - val_loss: 1.2905 - val_accuracy: 0.7090\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2686 - accuracy: 0.7261 - val_loss: 1.2878 - val_accuracy: 0.7030\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2646 - accuracy: 0.7261 - val_loss: 1.2884 - val_accuracy: 0.7030\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2609 - accuracy: 0.7257 - val_loss: 1.2837 - val_accuracy: 0.7030\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2574 - accuracy: 0.7261 - val_loss: 1.2791 - val_accuracy: 0.7040\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2539 - accuracy: 0.7265 - val_loss: 1.2736 - val_accuracy: 0.7080\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2501 - accuracy: 0.7260 - val_loss: 1.2689 - val_accuracy: 0.7040\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2466 - accuracy: 0.7268 - val_loss: 1.2671 - val_accuracy: 0.7050\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2434 - accuracy: 0.7276 - val_loss: 1.2667 - val_accuracy: 0.7070\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2395 - accuracy: 0.7277 - val_loss: 1.2614 - val_accuracy: 0.7030\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2364 - accuracy: 0.7279 - val_loss: 1.2556 - val_accuracy: 0.7070\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2333 - accuracy: 0.7301 - val_loss: 1.2578 - val_accuracy: 0.7100\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2299 - accuracy: 0.7293 - val_loss: 1.2532 - val_accuracy: 0.7060\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2261 - accuracy: 0.7284 - val_loss: 1.2452 - val_accuracy: 0.7060\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2226 - accuracy: 0.7285 - val_loss: 1.2447 - val_accuracy: 0.7060\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2197 - accuracy: 0.7271 - val_loss: 1.2405 - val_accuracy: 0.7020\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2167 - accuracy: 0.7283 - val_loss: 1.2372 - val_accuracy: 0.7050\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2136 - accuracy: 0.7304 - val_loss: 1.2348 - val_accuracy: 0.7060\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2102 - accuracy: 0.7296 - val_loss: 1.2337 - val_accuracy: 0.7130\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "L1_model = models.Sequential()\n",
    "\n",
    "# Add the input and first hidden layer\n",
    "L1_model.add(layers.Dense(50, activation='relu', kernel_regularizer= regularizers.l1(0.005),\n",
    "                         input_shape=(2000,)))\n",
    "\n",
    "# Add a hidden layer\n",
    "L1_model.add(layers.Dense(25, activation='relu', kernel_regularizer= regularizers.l1(0.005)))\n",
    "\n",
    "# Add an output layer\n",
    "L1_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "L1_model.compile(optimizer='SGD', \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# Train the model \n",
    "L1_model_val = L1_model.fit(X_train_tokens, \n",
    "                            y_train_lb, \n",
    "                            epochs=150, \n",
    "                            batch_size=256, \n",
    "                            validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training as well as the validation accuracy for the L1 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:52:20.416236Z",
     "start_time": "2020-05-12T14:52:20.249237Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-4f924245569e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mL1_model_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0macc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mval_acc_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL1_model_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsoAAAHWCAYAAABuaq89AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATLElEQVR4nO3dX4jld3nH8c9j1lTQqNDdgmQTE+immgYhdkgtXqiYliQXmxsrCYhVgnvTKK0iRBSVeFWlCEL8s6WSKmgavdBFVlKwKYoYyYa0wUQCS7RmiZD4LzeiMe3Ti5nKOHl25+x65sxm83rBwvzO+c6ZB/Jl5p3f/Ob8qrsDAAD8ruft9gAAAHA2EsoAADAQygAAMBDKAAAwEMoAADAQygAAMNg2lKvqs1X1eFV97yTPV1V9oqqOV9UDVfXq5Y8JAACrtcgZ5duTXHOK569NcmDj36Ekn/r9xwIAgN21bSh39zeT/OwUS65P8rled0+Sl1bVy5Y1IAAA7IZlXKN8YZJHNx2f2HgMAACetfYs4TVqeGy8L3ZVHcr65Rl54Qtf+GeveMUrlvDlAQDg5O67776fdPe+0/28ZYTyiSQXbTren+SxaWF3H05yOEnW1tb62LFjS/jyAABwclX132fyecu49OJIkrduvPvFa5I82d0/XsLrAgDArtn2jHJVfTHJ65PsraoTST6U5PlJ0t2fTnI0yXVJjif5ZZK379SwAACwKtuGcnffuM3zneRvlzYRAACcBdyZDwAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAYLhXJVXVNVD1fV8aq6ZXj+4qq6u6rur6oHquq65Y8KAACrs20oV9V5SW5Lcm2Sy5PcWFWXb1n2gSR3dveVSW5I8sllDwoAAKu0yBnlq5Ic7+5HuvupJHckuX7Lmk7y4o2PX5LkseWNCAAAq7dngTUXJnl00/GJJH++Zc2Hk/xbVb0zyQuTXL2U6QAAYJcscka5hsd6y/GNSW7v7v1Jrkvy+ap6xmtX1aGqOlZVx5544onTnxYAAFZkkVA+keSiTcf788xLK25KcmeSdPd3krwgyd6tL9Tdh7t7rbvX9u3bd2YTAwDACiwSyvcmOVBVl1bV+Vn/Y70jW9b8KMkbk6SqXpn1UHbKGACAZ61tQ7m7n05yc5K7knw/6+9u8WBV3VpVBzeWvSfJO6rqv5J8Mcnbunvr5RkAAPCsscgf86W7jyY5uuWxD276+KEkr13uaAAAsHvcmQ8AAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGC4VyVV1TVQ9X1fGquuUka95cVQ9V1YNV9YXljgkAAKu1Z7sFVXVektuS/GWSE0nuraoj3f3QpjUHkrwvyWu7++dV9Uc7NTAAAKzCImeUr0pyvLsf6e6nktyR5Pota96R5Lbu/nmSdPfjyx0TAABWa5FQvjDJo5uOT2w8ttllSS6rqm9X1T1Vdc2yBgQAgN2w7aUXSWp4rIfXOZDk9Un2J/lWVV3R3b/4nReqOpTkUJJcfPHFpz0sAACsyiJnlE8kuWjT8f4kjw1rvtrdv+nuHyR5OOvh/Du6+3B3r3X32r59+850ZgAA2HGLhPK9SQ5U1aVVdX6SG5Ic2bLmK0nekCRVtTfrl2I8ssxBAQBglbYN5e5+OsnNSe5K8v0kd3b3g1V1a1Ud3Fh2V5KfVtVDSe5O8t7u/ulODQ0AADuturdebrwaa2trfezYsV352gAAPHdU1X3dvXa6n+fOfAAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADBYKJSr6pqqeriqjlfVLadY96aq6qpaW96IAACwetuGclWdl+S2JNcmuTzJjVV1+bDugiTvSvLdZQ8JAACrtsgZ5auSHO/uR7r7qSR3JLl+WPeRJB9N8qslzgcAALtikVC+MMmjm45PbDz2W1V1ZZKLuvtrS5wNAAB2zSKhXMNj/dsnq56X5ONJ3rPtC1UdqqpjVXXsiSeeWHxKAABYsUVC+USSizYd70/y2KbjC5JckeQ/quqHSV6T5Mj0B33dfbi717p7bd++fWc+NQAA7LBFQvneJAeq6tKqOj/JDUmO/P+T3f1kd+/t7ku6+5Ik9yQ52N3HdmRiAABYgW1DubufTnJzkruSfD/Jnd39YFXdWlUHd3pAAADYDXsWWdTdR5Mc3fLYB0+y9vW//1gAALC73JkPAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABkIZAAAGQhkAAAZCGQAABguFclVdU1UPV9XxqrpleP7dVfVQVT1QVd+oqpcvf1QAAFidbUO5qs5LcluSa5NcnuTGqrp8y7L7k6x196uSfDnJR5c9KAAArNIiZ5SvSnK8ux/p7qeS3JHk+s0Luvvu7v7lxuE9SfYvd0wAAFitRUL5wiSPbjo+sfHYydyU5Ou/z1AAALDb9iywpobHelxY9ZYka0led5LnDyU5lCQXX3zxgiMCAMDqLXJG+USSizYd70/y2NZFVXV1kvcnOdjdv55eqLsPd/dad6/t27fvTOYFAICVWCSU701yoKourarzk9yQ5MjmBVV1ZZLPZD2SH1/+mAAAsFrbhnJ3P53k5iR3Jfl+kju7+8GqurWqDm4s+1iSFyX5UlX9Z1UdOcnLAQDAs8Ii1yinu48mObrlsQ9u+vjqJc8FAAC7yp35AABgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYCCUAQBgIJQBAGAglAEAYLBQKFfVNVX1cFUdr6pbhuf/oKr+deP571bVJcseFAAAVmnbUK6q85LcluTaJJcnubGqLt+y7KYkP+/uP07y8ST/sOxBAQBglRY5o3xVkuPd/Uh3P5XkjiTXb1lzfZJ/2fj4y0neWFW1vDEBAGC1FgnlC5M8uun4xMZj45rufjrJk0n+cBkDAgDAbtizwJrpzHCfwZpU1aEkhzYOf11V31vg6/PcsjfJT3Z7CM469gUT+4KJfcHkT87kkxYJ5RNJLtp0vD/JYydZc6Kq9iR5SZKfbX2h7j6c5HCSVNWx7l47k6E5d9kXTOwLJvYFE/uCSVUdO5PPW+TSi3uTHKiqS6vq/CQ3JDmyZc2RJH+z8fGbkvx7dz/jjDIAADxbbHtGubufrqqbk9yV5Lwkn+3uB6vq1iTHuvtIkn9O8vmqOp71M8k37OTQAACw0xa59CLdfTTJ0S2PfXDTx79K8ten+bUPn+Z6nhvsCyb2BRP7gol9weSM9kW5QgIAAJ7JLawBAGCw46Hs9tdMFtgX766qh6rqgar6RlW9fDfmZLW22xeb1r2pqrqq/GX7c8Ai+6Kq3rzxPePBqvrCqmdk9Rb4OXJxVd1dVfdv/Cy5bjfmZHWq6rNV9fjJ3n641n1iY888UFWv3u41dzSU3f6ayYL74v4ka939qqzf7fGjq52SVVtwX6SqLkjyriTfXe2E7IZF9kVVHUjyviSv7e4/TfJ3Kx+UlVrw+8UHktzZ3Vdm/U0GPrnaKdkFtye55hTPX5vkwMa/Q0k+td0L7vQZZbe/ZrLtvujuu7v7lxuH92T9/bs5ty3y/SJJPpL1/3H61SqHY9cssi/ekeS27v55knT34yuekdVbZF90khdvfPySPPMeEJxjuvubGe7jscn1ST7X6+5J8tKqetmpXnOnQ9ntr5kssi82uynJ13d0Is4G2+6LqroyyUXd/bVVDsauWuT7xWVJLquqb1fVPVV1qjNKnBsW2RcfTvKWqjqR9XfueudqRuMsdrr9sdjbw/0elnb7a84pC/83r6q3JFlL8rodnYizwSn3RVU9L+uXZ71tVQNxVljk+8WerP8q9fVZ/+3Tt6rqiu7+xQ7Pxu5ZZF/cmOT27v7HqvqLrN/v4Yru/t+dH4+z1Gk3506fUT6d21/nVLe/5pyyyL5IVV2d5P1JDnb3r1c0G7tnu31xQZIrkvxHVf0wyWuSHPEHfee8RX+OfLW7f9PdP0jycNbDmXPXIvvipiR3Jkl3fyfJC5LsXcl0nK0W6o/NdjqU3f6aybb7YuNX7J/JeiS73vC54ZT7oruf7O693X1Jd1+S9WvXD3b3sd0ZlxVZ5OfIV5K8IUmqam/WL8V4ZKVTsmqL7IsfJXljklTVK7Meyk+sdErONkeSvHXj3S9ek+TJ7v7xqT5hRy+9cPtrJgvui48leVGSL238beePuvvgrg3NjltwX/Acs+C+uCvJX1XVQ0n+J8l7u/unuzc1O23BffGeJP9UVX+f9V+vv82JuHNbVX0x65dg7d24Nv1DSZ6fJN396axfq35dkuNJfpnk7du+pj0DAADP5M58AAAwEMoAADAQygAAMBDKAAAwEMoAADAQygAAMBDKAAAwEMoAADD4P4fzu2ZTW1i9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "L1_model_dict = L1_model_val.history\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "ax.plot(epochs, acc_values, label='Training acc L1')\n",
    "ax.plot(epochs, val_acc_values, label='Validation acc L1')\n",
    "ax.set_title('Training & validation accuracy with L1 regularization')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy isn't still that good. Next, experiment with dropout regularization to see if it offers any advantages. \n",
    "\n",
    "\n",
    "## Dropout Regularization \n",
    "\n",
    "It's time to try another technique: applying dropout to layers. As discussed in the earlier lesson, this involves setting a certain proportion of units in each layer to zero. In the following cell: \n",
    "\n",
    "- Apply a dropout rate of 30% to the input layer \n",
    "- Add a first hidden layer with 50 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the first hidden layer \n",
    "- Add a second hidden layer with 25 units and `'relu'` activation \n",
    "- Apply a dropout rate of 30% to the second hidden layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:57:43.846547Z",
     "start_time": "2020-05-12T14:56:57.733380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/150\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.0084 - accuracy: 0.1349 - val_loss: 1.9508 - val_accuracy: 0.1460\n",
      "Epoch 2/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9679 - accuracy: 0.1488 - val_loss: 1.9310 - val_accuracy: 0.1780\n",
      "Epoch 3/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9487 - accuracy: 0.1571 - val_loss: 1.9194 - val_accuracy: 0.1940\n",
      "Epoch 4/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9379 - accuracy: 0.1740 - val_loss: 1.9086 - val_accuracy: 0.2290\n",
      "Epoch 5/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9235 - accuracy: 0.1868 - val_loss: 1.8969 - val_accuracy: 0.2360\n",
      "Epoch 6/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.9129 - accuracy: 0.1981 - val_loss: 1.8845 - val_accuracy: 0.2370\n",
      "Epoch 7/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.9009 - accuracy: 0.2155 - val_loss: 1.8710 - val_accuracy: 0.2390\n",
      "Epoch 8/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8880 - accuracy: 0.2211 - val_loss: 1.8553 - val_accuracy: 0.2480\n",
      "Epoch 9/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8759 - accuracy: 0.2328 - val_loss: 1.8377 - val_accuracy: 0.2620\n",
      "Epoch 10/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8594 - accuracy: 0.2483 - val_loss: 1.8188 - val_accuracy: 0.2800\n",
      "Epoch 11/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8497 - accuracy: 0.2544 - val_loss: 1.7969 - val_accuracy: 0.3050\n",
      "Epoch 12/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8306 - accuracy: 0.2655 - val_loss: 1.7764 - val_accuracy: 0.3410\n",
      "Epoch 13/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8217 - accuracy: 0.2697 - val_loss: 1.7548 - val_accuracy: 0.3430\n",
      "Epoch 14/150\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8041 - accuracy: 0.2771 - val_loss: 1.7318 - val_accuracy: 0.3510\n",
      "Epoch 15/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7868 - accuracy: 0.2911 - val_loss: 1.7085 - val_accuracy: 0.3870\n",
      "Epoch 16/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7649 - accuracy: 0.3039 - val_loss: 1.6828 - val_accuracy: 0.3900\n",
      "Epoch 17/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7516 - accuracy: 0.3052 - val_loss: 1.6591 - val_accuracy: 0.4050\n",
      "Epoch 18/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7284 - accuracy: 0.3164 - val_loss: 1.6335 - val_accuracy: 0.4140\n",
      "Epoch 19/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7141 - accuracy: 0.3239 - val_loss: 1.6081 - val_accuracy: 0.4280\n",
      "Epoch 20/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6923 - accuracy: 0.3443 - val_loss: 1.5843 - val_accuracy: 0.4570\n",
      "Epoch 21/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6821 - accuracy: 0.3336 - val_loss: 1.5587 - val_accuracy: 0.4660\n",
      "Epoch 22/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6567 - accuracy: 0.3616 - val_loss: 1.5298 - val_accuracy: 0.4850\n",
      "Epoch 23/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6431 - accuracy: 0.3653 - val_loss: 1.5058 - val_accuracy: 0.4970\n",
      "Epoch 24/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6260 - accuracy: 0.3700 - val_loss: 1.4808 - val_accuracy: 0.5240\n",
      "Epoch 25/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6014 - accuracy: 0.3805 - val_loss: 1.4549 - val_accuracy: 0.5270\n",
      "Epoch 26/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5951 - accuracy: 0.3755 - val_loss: 1.4339 - val_accuracy: 0.5460\n",
      "Epoch 27/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.5614 - accuracy: 0.3936 - val_loss: 1.4075 - val_accuracy: 0.5680\n",
      "Epoch 28/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5470 - accuracy: 0.4028 - val_loss: 1.3834 - val_accuracy: 0.5780\n",
      "Epoch 29/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5331 - accuracy: 0.4137 - val_loss: 1.3615 - val_accuracy: 0.5870\n",
      "Epoch 30/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5194 - accuracy: 0.4177 - val_loss: 1.3390 - val_accuracy: 0.6020\n",
      "Epoch 31/150\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5128 - accuracy: 0.4129 - val_loss: 1.3207 - val_accuracy: 0.6110\n",
      "Epoch 32/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4895 - accuracy: 0.4273 - val_loss: 1.2977 - val_accuracy: 0.6140\n",
      "Epoch 33/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4677 - accuracy: 0.4371 - val_loss: 1.2760 - val_accuracy: 0.6150\n",
      "Epoch 34/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4629 - accuracy: 0.4397 - val_loss: 1.2611 - val_accuracy: 0.6230\n",
      "Epoch 35/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4318 - accuracy: 0.4496 - val_loss: 1.2378 - val_accuracy: 0.6240\n",
      "Epoch 36/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4347 - accuracy: 0.4517 - val_loss: 1.2207 - val_accuracy: 0.6260\n",
      "Epoch 37/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4141 - accuracy: 0.4599 - val_loss: 1.2026 - val_accuracy: 0.6430\n",
      "Epoch 38/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3973 - accuracy: 0.4668 - val_loss: 1.1886 - val_accuracy: 0.6460\n",
      "Epoch 39/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3886 - accuracy: 0.4729 - val_loss: 1.1705 - val_accuracy: 0.6480\n",
      "Epoch 40/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.3781 - accuracy: 0.4685 - val_loss: 1.1553 - val_accuracy: 0.6520\n",
      "Epoch 41/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3606 - accuracy: 0.4857 - val_loss: 1.1389 - val_accuracy: 0.6600\n",
      "Epoch 42/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3667 - accuracy: 0.4807 - val_loss: 1.1299 - val_accuracy: 0.6630\n",
      "Epoch 43/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3513 - accuracy: 0.4819 - val_loss: 1.1141 - val_accuracy: 0.6650\n",
      "Epoch 44/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3303 - accuracy: 0.4957 - val_loss: 1.0987 - val_accuracy: 0.6620\n",
      "Epoch 45/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3125 - accuracy: 0.5043 - val_loss: 1.0817 - val_accuracy: 0.6680\n",
      "Epoch 46/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3008 - accuracy: 0.5025 - val_loss: 1.0712 - val_accuracy: 0.6670\n",
      "Epoch 47/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3047 - accuracy: 0.5117 - val_loss: 1.0646 - val_accuracy: 0.6680\n",
      "Epoch 48/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.3046 - accuracy: 0.4940 - val_loss: 1.0511 - val_accuracy: 0.6800\n",
      "Epoch 49/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2815 - accuracy: 0.5140 - val_loss: 1.0387 - val_accuracy: 0.6800\n",
      "Epoch 50/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2708 - accuracy: 0.5205 - val_loss: 1.0287 - val_accuracy: 0.6860\n",
      "Epoch 51/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2548 - accuracy: 0.5231 - val_loss: 1.0156 - val_accuracy: 0.6900\n",
      "Epoch 52/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2585 - accuracy: 0.5280 - val_loss: 1.0098 - val_accuracy: 0.6870\n",
      "Epoch 53/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2485 - accuracy: 0.5263 - val_loss: 0.9971 - val_accuracy: 0.6920\n",
      "Epoch 54/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2358 - accuracy: 0.5312 - val_loss: 0.9896 - val_accuracy: 0.6920\n",
      "Epoch 55/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2272 - accuracy: 0.5405 - val_loss: 0.9798 - val_accuracy: 0.6920\n",
      "Epoch 56/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2099 - accuracy: 0.5372 - val_loss: 0.9683 - val_accuracy: 0.6960\n",
      "Epoch 57/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2021 - accuracy: 0.5417 - val_loss: 0.9588 - val_accuracy: 0.6980\n",
      "Epoch 58/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1893 - accuracy: 0.5567 - val_loss: 0.9488 - val_accuracy: 0.6970\n",
      "Epoch 59/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1969 - accuracy: 0.5511 - val_loss: 0.9415 - val_accuracy: 0.7060\n",
      "Epoch 60/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1834 - accuracy: 0.5507 - val_loss: 0.9356 - val_accuracy: 0.7020\n",
      "Epoch 61/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.1781 - accuracy: 0.5572 - val_loss: 0.9312 - val_accuracy: 0.7070\n",
      "Epoch 62/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1723 - accuracy: 0.5551 - val_loss: 0.9211 - val_accuracy: 0.7040\n",
      "Epoch 63/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1512 - accuracy: 0.5604 - val_loss: 0.9103 - val_accuracy: 0.7090\n",
      "Epoch 64/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1582 - accuracy: 0.5520 - val_loss: 0.9065 - val_accuracy: 0.7110\n",
      "Epoch 65/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1623 - accuracy: 0.5667 - val_loss: 0.9034 - val_accuracy: 0.7050\n",
      "Epoch 66/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1368 - accuracy: 0.5744 - val_loss: 0.8941 - val_accuracy: 0.7080\n",
      "Epoch 67/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1362 - accuracy: 0.5725 - val_loss: 0.8872 - val_accuracy: 0.7090\n",
      "Epoch 68/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1302 - accuracy: 0.5748 - val_loss: 0.8810 - val_accuracy: 0.7080\n",
      "Epoch 69/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1172 - accuracy: 0.5792 - val_loss: 0.8735 - val_accuracy: 0.7110\n",
      "Epoch 70/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1109 - accuracy: 0.5803 - val_loss: 0.8663 - val_accuracy: 0.7120\n",
      "Epoch 71/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1084 - accuracy: 0.5872 - val_loss: 0.8629 - val_accuracy: 0.7090\n",
      "Epoch 72/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1199 - accuracy: 0.5820 - val_loss: 0.8584 - val_accuracy: 0.7150\n",
      "Epoch 73/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.1055 - accuracy: 0.5819 - val_loss: 0.8541 - val_accuracy: 0.7110\n",
      "Epoch 74/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1011 - accuracy: 0.5845 - val_loss: 0.8464 - val_accuracy: 0.7170\n",
      "Epoch 75/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0871 - accuracy: 0.5985 - val_loss: 0.8407 - val_accuracy: 0.7170\n",
      "Epoch 76/150\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0808 - accuracy: 0.59 - 0s 40us/step - loss: 1.0823 - accuracy: 0.5952 - val_loss: 0.8379 - val_accuracy: 0.7160\n",
      "Epoch 77/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0707 - accuracy: 0.5988 - val_loss: 0.8296 - val_accuracy: 0.7180\n",
      "Epoch 78/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0818 - accuracy: 0.5983 - val_loss: 0.8278 - val_accuracy: 0.7180\n",
      "Epoch 79/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0759 - accuracy: 0.6039 - val_loss: 0.8254 - val_accuracy: 0.7220\n",
      "Epoch 80/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0567 - accuracy: 0.6007 - val_loss: 0.8202 - val_accuracy: 0.7210\n",
      "Epoch 81/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0523 - accuracy: 0.5992 - val_loss: 0.8111 - val_accuracy: 0.7270\n",
      "Epoch 82/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0525 - accuracy: 0.6023 - val_loss: 0.8093 - val_accuracy: 0.7270\n",
      "Epoch 83/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0463 - accuracy: 0.5992 - val_loss: 0.8045 - val_accuracy: 0.7230\n",
      "Epoch 84/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0437 - accuracy: 0.6101 - val_loss: 0.7999 - val_accuracy: 0.7280\n",
      "Epoch 85/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0420 - accuracy: 0.6069 - val_loss: 0.7942 - val_accuracy: 0.7340\n",
      "Epoch 86/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0398 - accuracy: 0.6083 - val_loss: 0.7963 - val_accuracy: 0.7270\n",
      "Epoch 87/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0446 - accuracy: 0.6077 - val_loss: 0.7937 - val_accuracy: 0.7280\n",
      "Epoch 88/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0177 - accuracy: 0.6203 - val_loss: 0.7849 - val_accuracy: 0.7280\n",
      "Epoch 89/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0291 - accuracy: 0.6141 - val_loss: 0.7830 - val_accuracy: 0.7290\n",
      "Epoch 90/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0193 - accuracy: 0.6140 - val_loss: 0.7781 - val_accuracy: 0.7290\n",
      "Epoch 91/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0214 - accuracy: 0.6184 - val_loss: 0.7771 - val_accuracy: 0.7290\n",
      "Epoch 92/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.0048 - accuracy: 0.6268 - val_loss: 0.7710 - val_accuracy: 0.7310\n",
      "Epoch 93/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0157 - accuracy: 0.6261 - val_loss: 0.7716 - val_accuracy: 0.7330\n",
      "Epoch 94/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0110 - accuracy: 0.6201 - val_loss: 0.7684 - val_accuracy: 0.7330\n",
      "Epoch 95/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9939 - accuracy: 0.6260 - val_loss: 0.7612 - val_accuracy: 0.7340\n",
      "Epoch 96/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0068 - accuracy: 0.6256 - val_loss: 0.7600 - val_accuracy: 0.7320\n",
      "Epoch 97/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0012 - accuracy: 0.6284 - val_loss: 0.7573 - val_accuracy: 0.7320\n",
      "Epoch 98/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9750 - accuracy: 0.6263 - val_loss: 0.7527 - val_accuracy: 0.7300\n",
      "Epoch 99/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9820 - accuracy: 0.6352 - val_loss: 0.7498 - val_accuracy: 0.7300\n",
      "Epoch 100/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9870 - accuracy: 0.6331 - val_loss: 0.7500 - val_accuracy: 0.7290\n",
      "Epoch 101/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9667 - accuracy: 0.6396 - val_loss: 0.7461 - val_accuracy: 0.7300\n",
      "Epoch 102/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9775 - accuracy: 0.6351 - val_loss: 0.7429 - val_accuracy: 0.7310\n",
      "Epoch 103/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9579 - accuracy: 0.6484 - val_loss: 0.7393 - val_accuracy: 0.7320\n",
      "Epoch 104/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9592 - accuracy: 0.6393 - val_loss: 0.7379 - val_accuracy: 0.7340\n",
      "Epoch 105/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9736 - accuracy: 0.6451 - val_loss: 0.7388 - val_accuracy: 0.7320\n",
      "Epoch 106/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9658 - accuracy: 0.6393 - val_loss: 0.7338 - val_accuracy: 0.7320\n",
      "Epoch 107/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9577 - accuracy: 0.6357 - val_loss: 0.7300 - val_accuracy: 0.7330\n",
      "Epoch 108/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9532 - accuracy: 0.6407 - val_loss: 0.7277 - val_accuracy: 0.7330\n",
      "Epoch 109/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9502 - accuracy: 0.6367 - val_loss: 0.7260 - val_accuracy: 0.7330\n",
      "Epoch 110/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9479 - accuracy: 0.6523 - val_loss: 0.7234 - val_accuracy: 0.7330\n",
      "Epoch 111/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9416 - accuracy: 0.6491 - val_loss: 0.7186 - val_accuracy: 0.7370\n",
      "Epoch 112/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9418 - accuracy: 0.6487 - val_loss: 0.7199 - val_accuracy: 0.7320\n",
      "Epoch 113/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9273 - accuracy: 0.6503 - val_loss: 0.7146 - val_accuracy: 0.7340\n",
      "Epoch 114/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9402 - accuracy: 0.6475 - val_loss: 0.7122 - val_accuracy: 0.7350\n",
      "Epoch 115/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9506 - accuracy: 0.6469 - val_loss: 0.7107 - val_accuracy: 0.7350\n",
      "Epoch 116/150\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9214 - accuracy: 0.6568 - val_loss: 0.7095 - val_accuracy: 0.7370\n",
      "Epoch 117/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9220 - accuracy: 0.6577 - val_loss: 0.7104 - val_accuracy: 0.7350\n",
      "Epoch 118/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9151 - accuracy: 0.6716 - val_loss: 0.7057 - val_accuracy: 0.7380\n",
      "Epoch 119/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9194 - accuracy: 0.6603 - val_loss: 0.7035 - val_accuracy: 0.7380\n",
      "Epoch 120/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9169 - accuracy: 0.6637 - val_loss: 0.7032 - val_accuracy: 0.7390\n",
      "Epoch 121/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9330 - accuracy: 0.6579 - val_loss: 0.7016 - val_accuracy: 0.7370\n",
      "Epoch 122/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9058 - accuracy: 0.6683 - val_loss: 0.6992 - val_accuracy: 0.7410\n",
      "Epoch 123/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9029 - accuracy: 0.6589 - val_loss: 0.6968 - val_accuracy: 0.7430\n",
      "Epoch 124/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9074 - accuracy: 0.6601 - val_loss: 0.6954 - val_accuracy: 0.7380\n",
      "Epoch 125/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9073 - accuracy: 0.6631 - val_loss: 0.6948 - val_accuracy: 0.7380\n",
      "Epoch 126/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8860 - accuracy: 0.6773 - val_loss: 0.6898 - val_accuracy: 0.7400\n",
      "Epoch 127/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9013 - accuracy: 0.6679 - val_loss: 0.6892 - val_accuracy: 0.7410\n",
      "Epoch 128/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8973 - accuracy: 0.6705 - val_loss: 0.6902 - val_accuracy: 0.7400\n",
      "Epoch 129/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8862 - accuracy: 0.6719 - val_loss: 0.6855 - val_accuracy: 0.7400\n",
      "Epoch 130/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8976 - accuracy: 0.6679 - val_loss: 0.6856 - val_accuracy: 0.7380\n",
      "Epoch 131/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8883 - accuracy: 0.6661 - val_loss: 0.6819 - val_accuracy: 0.7430\n",
      "Epoch 132/150\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8867 - accuracy: 0.6720 - val_loss: 0.6814 - val_accuracy: 0.7430\n",
      "Epoch 133/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8733 - accuracy: 0.6829 - val_loss: 0.6787 - val_accuracy: 0.7430\n",
      "Epoch 134/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8770 - accuracy: 0.6731 - val_loss: 0.6786 - val_accuracy: 0.7440\n",
      "Epoch 135/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8833 - accuracy: 0.6741 - val_loss: 0.6794 - val_accuracy: 0.7410\n",
      "Epoch 136/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8750 - accuracy: 0.6717 - val_loss: 0.6751 - val_accuracy: 0.7420\n",
      "Epoch 137/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8738 - accuracy: 0.6771 - val_loss: 0.6766 - val_accuracy: 0.7430\n",
      "Epoch 138/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8688 - accuracy: 0.6725 - val_loss: 0.6749 - val_accuracy: 0.7420\n",
      "Epoch 139/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8669 - accuracy: 0.6769 - val_loss: 0.6712 - val_accuracy: 0.7430\n",
      "Epoch 140/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8578 - accuracy: 0.6819 - val_loss: 0.6696 - val_accuracy: 0.7420\n",
      "Epoch 141/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8844 - accuracy: 0.6773 - val_loss: 0.6701 - val_accuracy: 0.7470\n",
      "Epoch 142/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8685 - accuracy: 0.6797 - val_loss: 0.6684 - val_accuracy: 0.7460\n",
      "Epoch 143/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8604 - accuracy: 0.6769 - val_loss: 0.6669 - val_accuracy: 0.7450\n",
      "Epoch 144/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8560 - accuracy: 0.6775 - val_loss: 0.6662 - val_accuracy: 0.7460\n",
      "Epoch 145/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8498 - accuracy: 0.6843 - val_loss: 0.6653 - val_accuracy: 0.7470\n",
      "Epoch 146/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8408 - accuracy: 0.6889 - val_loss: 0.6643 - val_accuracy: 0.7440\n",
      "Epoch 147/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8525 - accuracy: 0.6811 - val_loss: 0.6632 - val_accuracy: 0.7470\n",
      "Epoch 148/150\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8438 - accuracy: 0.6885 - val_loss: 0.6600 - val_accuracy: 0.7470\n",
      "Epoch 149/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8433 - accuracy: 0.6785 - val_loss: 0.6624 - val_accuracy: 0.7430\n",
      "Epoch 150/150\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8523 - accuracy: 0.6848 - val_loss: 0.6612 - val_accuracy: 0.7470\n"
     ]
    }
   ],
   "source": [
    "#  This cell may take about a minute to run\n",
    "random.seed(123)\n",
    "dropout_model = models.Sequential()\n",
    "\n",
    "# Implement dropout to the input layer\n",
    "# NOTE: This is where you define the number of units in the input layer\n",
    "dropout_model.add(layers.Dropout(0.3, input_shape=(2000, )))\n",
    "\n",
    "# Add the first hidden layer\n",
    "dropout_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Implement dropout to the first hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "dropout_model.add(layers.Dense(25, activation='relu'))\n",
    "\n",
    "# Implement dropout to the second hidden layer \n",
    "dropout_model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Add the output layer\n",
    "dropout_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "dropout_model.compile(optimizer='SGD', \n",
    "                      loss='categorical_crossentropy', \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "dropout_model_val = dropout_model.fit(X_train_tokens, \n",
    "                                      y_train_lb, \n",
    "                                      epochs=150, \n",
    "                                      batch_size=256, \n",
    "                                      validation_data=(X_val_tokens, y_val_lb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:58:31.030395Z",
     "start_time": "2020-05-12T14:58:30.795390Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step\n",
      "Training Loss: 0.553 \n",
      "Training Accuracy: 0.81\n",
      "----------\n",
      "1500/1500 [==============================] - 0s 24us/step\n",
      "Test Loss: 0.615 \n",
      "Test Accuracy: 0.782\n"
     ]
    }
   ],
   "source": [
    "results_train = dropout_model.evaluate(X_train_tokens, y_train_lb)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = dropout_model.evaluate(X_test_tokens, y_test_lb)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again, and the training and test accuracy are very close!  \n",
    "\n",
    "## Bigger Data? \n",
    "\n",
    "Finally, let's examine if we can improve the model's performance just by adding more data. We've quadrapled the sample dataset from 10,000 to 40,000 observations, and all you need to do is run the code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:59:00.614071Z",
     "start_time": "2020-05-12T14:58:43.084112Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bigger_sample = df.sample(40000, random_state=123)\n",
    "\n",
    "X = df['Consumer complaint narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Train-test split\n",
    "X_train_bigger, X_test_bigger, y_train_bigger, y_test_bigger = train_test_split(X, \n",
    "                                                                                y, \n",
    "                                                                                test_size=6000, \n",
    "                                                                                random_state=42)\n",
    "\n",
    "# Validation set\n",
    "X_train_final_bigger, X_val_bigger, y_train_final_bigger, y_val_bigger = train_test_split(X_train_bigger, \n",
    "                                                                                          y_train_bigger, \n",
    "                                                                                          test_size=4000, \n",
    "                                                                                          random_state=42)\n",
    "\n",
    "\n",
    "# One-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(X_train_final_bigger)\n",
    "\n",
    "X_train_tokens_bigger = tokenizer.texts_to_matrix(X_train_final_bigger, mode='binary')\n",
    "X_val_tokens_bigger = tokenizer.texts_to_matrix(X_val_bigger, mode='binary')\n",
    "X_test_tokens_bigger = tokenizer.texts_to_matrix(X_test_bigger, mode='binary')\n",
    "\n",
    "# One-hot encoding of products\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train_final_bigger)\n",
    "\n",
    "y_train_lb_bigger = to_categorical(lb.transform(y_train_final_bigger))[:, :, 1]\n",
    "y_val_lb_bigger = to_categorical(lb.transform(y_val_bigger))[:, :, 1]\n",
    "y_test_lb_bigger = to_categorical(lb.transform(y_test_bigger))[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T15:02:07.575159Z",
     "start_time": "2020-05-12T14:59:06.716010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 4000 samples\n",
      "Epoch 1/150\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 1.8926 - accuracy: 0.2344 - val_loss: 1.8295 - val_accuracy: 0.3265\n",
      "Epoch 2/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 1.7115 - accuracy: 0.4148 - val_loss: 1.5912 - val_accuracy: 0.4782\n",
      "Epoch 3/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 1.4483 - accuracy: 0.5399 - val_loss: 1.3290 - val_accuracy: 0.5880\n",
      "Epoch 4/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 1.2111 - accuracy: 0.6209 - val_loss: 1.1197 - val_accuracy: 0.6470\n",
      "Epoch 5/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 1.0319 - accuracy: 0.6665 - val_loss: 0.9713 - val_accuracy: 0.6790\n",
      "Epoch 6/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.9075 - accuracy: 0.6928 - val_loss: 0.8709 - val_accuracy: 0.7028\n",
      "Epoch 7/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.8225 - accuracy: 0.7136 - val_loss: 0.8033 - val_accuracy: 0.7210\n",
      "Epoch 8/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.7628 - accuracy: 0.7284 - val_loss: 0.7552 - val_accuracy: 0.7358\n",
      "Epoch 9/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.7196 - accuracy: 0.7389 - val_loss: 0.7209 - val_accuracy: 0.7427\n",
      "Epoch 10/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.6863 - accuracy: 0.7503 - val_loss: 0.6951 - val_accuracy: 0.7480\n",
      "Epoch 11/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.6599 - accuracy: 0.7584 - val_loss: 0.6740 - val_accuracy: 0.7565\n",
      "Epoch 12/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.6387 - accuracy: 0.7650 - val_loss: 0.6577 - val_accuracy: 0.7628\n",
      "Epoch 13/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.6206 - accuracy: 0.7712 - val_loss: 0.6440 - val_accuracy: 0.7653\n",
      "Epoch 14/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.6053 - accuracy: 0.7772 - val_loss: 0.6334 - val_accuracy: 0.7685\n",
      "Epoch 15/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5922 - accuracy: 0.7809 - val_loss: 0.6251 - val_accuracy: 0.7707\n",
      "Epoch 16/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5803 - accuracy: 0.7855 - val_loss: 0.6135 - val_accuracy: 0.7730\n",
      "Epoch 17/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5694 - accuracy: 0.7903 - val_loss: 0.6067 - val_accuracy: 0.7753\n",
      "Epoch 18/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5598 - accuracy: 0.7937 - val_loss: 0.5990 - val_accuracy: 0.7790\n",
      "Epoch 19/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5509 - accuracy: 0.7983 - val_loss: 0.5939 - val_accuracy: 0.7818\n",
      "Epoch 20/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5427 - accuracy: 0.8006 - val_loss: 0.5887 - val_accuracy: 0.7862\n",
      "Epoch 21/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5350 - accuracy: 0.8031 - val_loss: 0.5853 - val_accuracy: 0.7872\n",
      "Epoch 22/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5279 - accuracy: 0.8063 - val_loss: 0.5829 - val_accuracy: 0.7865\n",
      "Epoch 23/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5213 - accuracy: 0.8086 - val_loss: 0.5756 - val_accuracy: 0.7895\n",
      "Epoch 24/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5147 - accuracy: 0.8112 - val_loss: 0.5744 - val_accuracy: 0.7920\n",
      "Epoch 25/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5086 - accuracy: 0.8141 - val_loss: 0.5710 - val_accuracy: 0.7960\n",
      "Epoch 26/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.5030 - accuracy: 0.8164 - val_loss: 0.5673 - val_accuracy: 0.7958\n",
      "Epoch 27/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4976 - accuracy: 0.8189 - val_loss: 0.5635 - val_accuracy: 0.7987\n",
      "Epoch 28/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4927 - accuracy: 0.8220 - val_loss: 0.5621 - val_accuracy: 0.8000\n",
      "Epoch 29/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4879 - accuracy: 0.8239 - val_loss: 0.5592 - val_accuracy: 0.7960\n",
      "Epoch 30/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4832 - accuracy: 0.8251 - val_loss: 0.5569 - val_accuracy: 0.8023\n",
      "Epoch 31/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4788 - accuracy: 0.8275 - val_loss: 0.5576 - val_accuracy: 0.8005\n",
      "Epoch 32/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4744 - accuracy: 0.8289 - val_loss: 0.5537 - val_accuracy: 0.7997\n",
      "Epoch 33/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.4705 - accuracy: 0.8306 - val_loss: 0.5501 - val_accuracy: 0.8045\n",
      "Epoch 34/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.4661 - accuracy: 0.8332 - val_loss: 0.5475 - val_accuracy: 0.8020\n",
      "Epoch 35/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.4627 - accuracy: 0.8340 - val_loss: 0.5481 - val_accuracy: 0.8027\n",
      "Epoch 36/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4590 - accuracy: 0.8349 - val_loss: 0.5463 - val_accuracy: 0.8033\n",
      "Epoch 37/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4555 - accuracy: 0.8371 - val_loss: 0.5464 - val_accuracy: 0.8020\n",
      "Epoch 38/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.4522 - accuracy: 0.8388 - val_loss: 0.5446 - val_accuracy: 0.8037\n",
      "Epoch 39/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4492 - accuracy: 0.8390 - val_loss: 0.5426 - val_accuracy: 0.8027\n",
      "Epoch 40/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4459 - accuracy: 0.8407 - val_loss: 0.5434 - val_accuracy: 0.8030\n",
      "Epoch 41/150\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.4430 - accuracy: 0.8413 - val_loss: 0.5420 - val_accuracy: 0.8052\n",
      "Epoch 42/150\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.4397 - accuracy: 0.8424 - val_loss: 0.5411 - val_accuracy: 0.8043\n",
      "Epoch 43/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4368 - accuracy: 0.8437 - val_loss: 0.5395 - val_accuracy: 0.8050\n",
      "Epoch 44/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4342 - accuracy: 0.8457 - val_loss: 0.5401 - val_accuracy: 0.8055\n",
      "Epoch 45/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4317 - accuracy: 0.8458 - val_loss: 0.5381 - val_accuracy: 0.8037\n",
      "Epoch 46/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4291 - accuracy: 0.8465 - val_loss: 0.5384 - val_accuracy: 0.8025\n",
      "Epoch 47/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.4266 - accuracy: 0.8479 - val_loss: 0.5356 - val_accuracy: 0.8037\n",
      "Epoch 48/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4242 - accuracy: 0.8487 - val_loss: 0.5379 - val_accuracy: 0.8058\n",
      "Epoch 49/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4219 - accuracy: 0.8499 - val_loss: 0.5367 - val_accuracy: 0.8055\n",
      "Epoch 50/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4193 - accuracy: 0.8513 - val_loss: 0.5375 - val_accuracy: 0.8040\n",
      "Epoch 51/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4170 - accuracy: 0.8522 - val_loss: 0.5369 - val_accuracy: 0.8043\n",
      "Epoch 52/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4150 - accuracy: 0.8525 - val_loss: 0.5375 - val_accuracy: 0.8023\n",
      "Epoch 53/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4127 - accuracy: 0.8544 - val_loss: 0.5384 - val_accuracy: 0.8050\n",
      "Epoch 54/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4107 - accuracy: 0.8551 - val_loss: 0.5388 - val_accuracy: 0.8055\n",
      "Epoch 55/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4088 - accuracy: 0.8560 - val_loss: 0.5377 - val_accuracy: 0.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4066 - accuracy: 0.8569 - val_loss: 0.5415 - val_accuracy: 0.8090\n",
      "Epoch 57/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4045 - accuracy: 0.8573 - val_loss: 0.5394 - val_accuracy: 0.8112\n",
      "Epoch 58/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4027 - accuracy: 0.8572 - val_loss: 0.5456 - val_accuracy: 0.8070\n",
      "Epoch 59/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.4006 - accuracy: 0.8581 - val_loss: 0.5357 - val_accuracy: 0.8100\n",
      "Epoch 60/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3993 - accuracy: 0.8594 - val_loss: 0.5363 - val_accuracy: 0.8067\n",
      "Epoch 61/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3974 - accuracy: 0.8596 - val_loss: 0.5360 - val_accuracy: 0.8100\n",
      "Epoch 62/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3957 - accuracy: 0.8605 - val_loss: 0.5383 - val_accuracy: 0.8062\n",
      "Epoch 63/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3939 - accuracy: 0.8616 - val_loss: 0.5346 - val_accuracy: 0.8108\n",
      "Epoch 64/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3923 - accuracy: 0.8620 - val_loss: 0.5354 - val_accuracy: 0.8075\n",
      "Epoch 65/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3902 - accuracy: 0.8625 - val_loss: 0.5347 - val_accuracy: 0.8105\n",
      "Epoch 66/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3889 - accuracy: 0.8625 - val_loss: 0.5393 - val_accuracy: 0.8060\n",
      "Epoch 67/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3872 - accuracy: 0.8633 - val_loss: 0.5359 - val_accuracy: 0.8130\n",
      "Epoch 68/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3860 - accuracy: 0.8639 - val_loss: 0.5380 - val_accuracy: 0.8108\n",
      "Epoch 69/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3842 - accuracy: 0.8653 - val_loss: 0.5365 - val_accuracy: 0.8117\n",
      "Epoch 70/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3824 - accuracy: 0.8653 - val_loss: 0.5405 - val_accuracy: 0.8148\n",
      "Epoch 71/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3814 - accuracy: 0.8660 - val_loss: 0.5364 - val_accuracy: 0.8123\n",
      "Epoch 72/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3800 - accuracy: 0.8672 - val_loss: 0.5396 - val_accuracy: 0.8102\n",
      "Epoch 73/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3783 - accuracy: 0.8679 - val_loss: 0.5357 - val_accuracy: 0.8155\n",
      "Epoch 74/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3767 - accuracy: 0.8676 - val_loss: 0.5406 - val_accuracy: 0.8115\n",
      "Epoch 75/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3752 - accuracy: 0.8678 - val_loss: 0.5373 - val_accuracy: 0.8110\n",
      "Epoch 76/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3742 - accuracy: 0.8685 - val_loss: 0.5408 - val_accuracy: 0.8133\n",
      "Epoch 77/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3731 - accuracy: 0.8692 - val_loss: 0.5438 - val_accuracy: 0.8108\n",
      "Epoch 78/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3714 - accuracy: 0.8690 - val_loss: 0.5393 - val_accuracy: 0.8142\n",
      "Epoch 79/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3701 - accuracy: 0.8694 - val_loss: 0.5475 - val_accuracy: 0.8110\n",
      "Epoch 80/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3690 - accuracy: 0.8698 - val_loss: 0.5485 - val_accuracy: 0.8033\n",
      "Epoch 81/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3677 - accuracy: 0.8704 - val_loss: 0.5430 - val_accuracy: 0.8130\n",
      "Epoch 82/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3664 - accuracy: 0.8710 - val_loss: 0.5438 - val_accuracy: 0.8058\n",
      "Epoch 83/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3653 - accuracy: 0.8717 - val_loss: 0.5406 - val_accuracy: 0.8080\n",
      "Epoch 84/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3640 - accuracy: 0.8719 - val_loss: 0.5434 - val_accuracy: 0.8140\n",
      "Epoch 85/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3629 - accuracy: 0.8724 - val_loss: 0.5444 - val_accuracy: 0.8138\n",
      "Epoch 86/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3615 - accuracy: 0.8724 - val_loss: 0.5447 - val_accuracy: 0.8110\n",
      "Epoch 87/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3605 - accuracy: 0.8726 - val_loss: 0.5556 - val_accuracy: 0.8075\n",
      "Epoch 88/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3594 - accuracy: 0.8738 - val_loss: 0.5510 - val_accuracy: 0.8100\n",
      "Epoch 89/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3581 - accuracy: 0.8743 - val_loss: 0.5441 - val_accuracy: 0.8110\n",
      "Epoch 90/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3570 - accuracy: 0.8737 - val_loss: 0.5477 - val_accuracy: 0.8092\n",
      "Epoch 91/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3557 - accuracy: 0.8745 - val_loss: 0.5500 - val_accuracy: 0.8150\n",
      "Epoch 92/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3550 - accuracy: 0.8753 - val_loss: 0.5518 - val_accuracy: 0.8158\n",
      "Epoch 93/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3537 - accuracy: 0.8753 - val_loss: 0.5486 - val_accuracy: 0.8077\n",
      "Epoch 94/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3525 - accuracy: 0.8762 - val_loss: 0.5486 - val_accuracy: 0.8138\n",
      "Epoch 95/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3514 - accuracy: 0.8764 - val_loss: 0.5584 - val_accuracy: 0.8067\n",
      "Epoch 96/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3507 - accuracy: 0.8769 - val_loss: 0.5512 - val_accuracy: 0.8090\n",
      "Epoch 97/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3497 - accuracy: 0.8769 - val_loss: 0.5538 - val_accuracy: 0.8117\n",
      "Epoch 98/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3489 - accuracy: 0.8766 - val_loss: 0.5492 - val_accuracy: 0.8100\n",
      "Epoch 99/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3473 - accuracy: 0.8772 - val_loss: 0.5527 - val_accuracy: 0.8120\n",
      "Epoch 100/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3462 - accuracy: 0.8780 - val_loss: 0.5503 - val_accuracy: 0.8120\n",
      "Epoch 101/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3457 - accuracy: 0.8781 - val_loss: 0.5535 - val_accuracy: 0.8083\n",
      "Epoch 102/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3447 - accuracy: 0.8770 - val_loss: 0.5579 - val_accuracy: 0.8095\n",
      "Epoch 103/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3434 - accuracy: 0.8796 - val_loss: 0.5506 - val_accuracy: 0.8110\n",
      "Epoch 104/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3428 - accuracy: 0.8787 - val_loss: 0.5538 - val_accuracy: 0.8112\n",
      "Epoch 105/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3416 - accuracy: 0.8787 - val_loss: 0.5535 - val_accuracy: 0.8065\n",
      "Epoch 106/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3405 - accuracy: 0.8800 - val_loss: 0.5614 - val_accuracy: 0.8080\n",
      "Epoch 107/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3398 - accuracy: 0.8799 - val_loss: 0.5604 - val_accuracy: 0.8083\n",
      "Epoch 108/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3384 - accuracy: 0.8801 - val_loss: 0.5552 - val_accuracy: 0.8098\n",
      "Epoch 109/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3376 - accuracy: 0.8807 - val_loss: 0.5586 - val_accuracy: 0.8125\n",
      "Epoch 110/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3369 - accuracy: 0.8808 - val_loss: 0.5597 - val_accuracy: 0.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3358 - accuracy: 0.8810 - val_loss: 0.5678 - val_accuracy: 0.8035\n",
      "Epoch 112/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3351 - accuracy: 0.8817 - val_loss: 0.5600 - val_accuracy: 0.8085\n",
      "Epoch 113/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3339 - accuracy: 0.8823 - val_loss: 0.5621 - val_accuracy: 0.8098\n",
      "Epoch 114/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3328 - accuracy: 0.8829 - val_loss: 0.5653 - val_accuracy: 0.8125\n",
      "Epoch 115/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3322 - accuracy: 0.8833 - val_loss: 0.5667 - val_accuracy: 0.8108\n",
      "Epoch 116/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3314 - accuracy: 0.8829 - val_loss: 0.5626 - val_accuracy: 0.8090\n",
      "Epoch 117/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3302 - accuracy: 0.8843 - val_loss: 0.5603 - val_accuracy: 0.8098\n",
      "Epoch 118/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3296 - accuracy: 0.8832 - val_loss: 0.5654 - val_accuracy: 0.8080\n",
      "Epoch 119/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3288 - accuracy: 0.8845 - val_loss: 0.5635 - val_accuracy: 0.8115\n",
      "Epoch 120/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3279 - accuracy: 0.8844 - val_loss: 0.5641 - val_accuracy: 0.8090\n",
      "Epoch 121/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3271 - accuracy: 0.8844 - val_loss: 0.5741 - val_accuracy: 0.8045\n",
      "Epoch 122/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3261 - accuracy: 0.8850 - val_loss: 0.5715 - val_accuracy: 0.8138\n",
      "Epoch 123/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3251 - accuracy: 0.8851 - val_loss: 0.5705 - val_accuracy: 0.8077\n",
      "Epoch 124/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3244 - accuracy: 0.8857 - val_loss: 0.5730 - val_accuracy: 0.8087\n",
      "Epoch 125/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3233 - accuracy: 0.8859 - val_loss: 0.5742 - val_accuracy: 0.8108\n",
      "Epoch 126/150\n",
      "50000/50000 [==============================] - 1s 23us/step - loss: 0.3225 - accuracy: 0.8863 - val_loss: 0.5713 - val_accuracy: 0.8062\n",
      "Epoch 127/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3216 - accuracy: 0.8869 - val_loss: 0.5736 - val_accuracy: 0.8050\n",
      "Epoch 128/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3207 - accuracy: 0.8871 - val_loss: 0.5728 - val_accuracy: 0.8105\n",
      "Epoch 129/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3198 - accuracy: 0.8881 - val_loss: 0.5698 - val_accuracy: 0.8070\n",
      "Epoch 130/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3190 - accuracy: 0.8882 - val_loss: 0.5830 - val_accuracy: 0.8055\n",
      "Epoch 131/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3182 - accuracy: 0.8880 - val_loss: 0.5758 - val_accuracy: 0.8108\n",
      "Epoch 132/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3170 - accuracy: 0.8878 - val_loss: 0.5760 - val_accuracy: 0.8123\n",
      "Epoch 133/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3162 - accuracy: 0.8894 - val_loss: 0.5757 - val_accuracy: 0.8127\n",
      "Epoch 134/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3155 - accuracy: 0.8890 - val_loss: 0.5769 - val_accuracy: 0.8117\n",
      "Epoch 135/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3146 - accuracy: 0.8889 - val_loss: 0.5866 - val_accuracy: 0.8067\n",
      "Epoch 136/150\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3139 - accuracy: 0.8888 - val_loss: 0.5940 - val_accuracy: 0.8010\n",
      "Epoch 137/150\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3129 - accuracy: 0.8902 - val_loss: 0.5806 - val_accuracy: 0.8095\n",
      "Epoch 138/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3122 - accuracy: 0.8895 - val_loss: 0.5808 - val_accuracy: 0.8058\n",
      "Epoch 139/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3112 - accuracy: 0.8904 - val_loss: 0.5983 - val_accuracy: 0.8025\n",
      "Epoch 140/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3106 - accuracy: 0.8906 - val_loss: 0.5802 - val_accuracy: 0.8087\n",
      "Epoch 141/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3095 - accuracy: 0.8911 - val_loss: 0.5906 - val_accuracy: 0.8077\n",
      "Epoch 142/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3086 - accuracy: 0.8917 - val_loss: 0.5926 - val_accuracy: 0.8067\n",
      "Epoch 143/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3078 - accuracy: 0.8921 - val_loss: 0.5859 - val_accuracy: 0.8083\n",
      "Epoch 144/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3069 - accuracy: 0.8925 - val_loss: 0.5848 - val_accuracy: 0.8115\n",
      "Epoch 145/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3058 - accuracy: 0.8924 - val_loss: 0.5896 - val_accuracy: 0.8077\n",
      "Epoch 146/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3054 - accuracy: 0.8926 - val_loss: 0.5886 - val_accuracy: 0.8045\n",
      "Epoch 147/150\n",
      "50000/50000 [==============================] - 1s 27us/step - loss: 0.3047 - accuracy: 0.8935 - val_loss: 0.5864 - val_accuracy: 0.8102\n",
      "Epoch 148/150\n",
      "50000/50000 [==============================] - 1s 26us/step - loss: 0.3035 - accuracy: 0.8936 - val_loss: 0.5885 - val_accuracy: 0.8055\n",
      "Epoch 149/150\n",
      "50000/50000 [==============================] - 1s 25us/step - loss: 0.3029 - accuracy: 0.8934 - val_loss: 0.5957 - val_accuracy: 0.8067\n",
      "Epoch 150/150\n",
      "50000/50000 [==============================] - 1s 24us/step - loss: 0.3022 - accuracy: 0.8927 - val_loss: 0.6036 - val_accuracy: 0.8037\n"
     ]
    }
   ],
   "source": [
    "#  This cell may take several minutes to run\n",
    "random.seed(123)\n",
    "bigger_data_model = models.Sequential()\n",
    "bigger_data_model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "bigger_data_model.add(layers.Dense(25, activation='relu'))\n",
    "bigger_data_model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "bigger_data_model.compile(optimizer='SGD', \n",
    "                          loss='categorical_crossentropy', \n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "bigger_data_model_val = bigger_data_model.fit(X_train_tokens_bigger,  \n",
    "                                              y_train_lb_bigger,  \n",
    "                                              epochs=150,  \n",
    "                                              batch_size=256,  \n",
    "                                              validation_data=(X_val_tokens_bigger, y_val_lb_bigger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T15:02:08.731172Z",
     "start_time": "2020-05-12T15:02:07.577158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 1s 21us/step\n",
      "Training Loss: 0.305 \n",
      "Training Accuracy: 0.891\n",
      "----------\n",
      "4000/4000 [==============================] - 0s 21us/step\n",
      "Test Loss: 0.604 \n",
      "Test Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "results_train = bigger_data_model.evaluate(X_train_tokens_bigger, y_train_lb_bigger)\n",
    "print(f'Training Loss: {results_train[0]:.3} \\nTraining Accuracy: {results_train[1]:.3}')\n",
    "\n",
    "print('----------')\n",
    "\n",
    "results_test = bigger_data_model.evaluate(X_val_tokens_bigger, y_val_lb_bigger)\n",
    "print(f'Test Loss: {results_test[0]:.3} \\nTest Accuracy: {results_test[1]:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs and no regularization technique, you were able to get both better test accuracy and loss. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance! \n",
    "\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "\n",
    "\n",
    "## Summary  \n",
    "\n",
    "In this lesson, you built deep learning models using a validation set and used several techniques such as L2 and L1 regularization, dropout regularization, and early stopping to improve the accuracy of your models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
